= Praktične vježbe

Dobrodošli u praktični dio!

U ovoj ću sekciji podijeliti svoje osobno iskustvo s postavljanjem, konfiguracijom i upravljanjem replikacijskim setovima. Sve vježbe izvodio sam lokalno koristeći Docker i Docker Compose, što mi je omogućilo da simuliram produkcijsko okruženje bez utjecaja na svoj sustav.

Zapamti, iako sam ja pratio preporučene korake, ovo je tvoje igralište za učenje. Uvijek postoji više načina da se dođe do istog cilja, pa te ohrabrujem da istražuješ i druge opcije koje smatraš boljima ili zanimljivijima.

IMPORTANT: Logovi su ti najbolji prijatelj.

== 1. MySQL asinkrona replikacija

Za bolje razumijevanje asinkrone replikacije, možete pogledati ovaj link:https://www.digitalocean.com/community/tutorials/how-to-set-up-replication-in-mysql[Digital Ocean tutorial] (nije namijenjen za Docker, ali je vrlo koristan).


Započnite s pripremom projektnog direktorija. Kreirajte novi folder u vašem **~** direktoriju i nazovite ga npr. **DockerAR**.
Kada pokrenete **docker-compose**, ime foldera će automatski postati naziv link:https://docs.docker.com/compose/how-tos/project-name/[projekta].
Kreiramo datoteku config i u njoj pravimo master.cnf i slave.cnf sa varijablama potrebnim za asinkronu replikaciju.
[source, cnf]
----
# Na master serveru
[mysqld]
log_bin = mysql_bin
server_id = 1
binlog_format = ROW
expire_logs_days = 7
# Na slave serveru
[mysqld]
server_id = 2
relay_log = mysql_relay_bin
log_replica_updates = ON 
----

I još kreiramo foldere za logove(logmaster i logslave).
Da bi mysql iz kontenjera mogao u njih pisati i čitati moramo mu dati potrebne dozvole.(chown i chmod)
Koja grupa je mysql možemo saznati pomoću komande

``docker exec --user mysql <ime_kontejnera> id``

``sudo chown -R 1001:1001 master_data``

Zatim kreirajte file **docker-compose.yaml** unutar tog foldera. Dodajte sljedeći sadržaj za definiranje servisa:

[source, yaml]
----
services:
  master:              # definiramo prvi kontenjer
    image: mysql:8.0  # vucemo image s docker huba s tagom 8.0
    container_name: master  # dajemo ime kontenjeru za lakse referenciranje
    ports:
      -  "33063:3306"  # exposamo port 3306 iz kontenjera na port 33063 na hostu
    volumes:                             # stvaramo *bind* volumene
      -  ./master_data:/var/lib/mysql    # bindamo mysql datu na hosta da ostane persistana
      -  ./config/master.cnf:/etc/mysql/conf.d/master.cnf  # bindamo mu da koristi nasu custom konfiguraciju
      -  ./logmaster:/var/log/mysql      # bindamo logove da ih mozemo citati s hosta
    environment:
      MYSQL_ROOT_PASSWORD: 1234         # root useru dajemo password
  slave:             # definiramo drugi kontenjer
    image: mysql:8.0      
    container_name: slave
    ports:
      -  "33064:3306"
    volumes:
      -  ./slave_data:/var/lib/mysql
      -  ./config/slave.cnf:/etc/mysql/conf.d/slave.cnf
      -  ./logslave:/var/log/mysql
    environment:
      MYSQL_ROOT_PASSWORD: 4321
    depends_on:                  # da se ne pokrece ako master nije pokrenut
      -  master
----

TIP: Vrste link:https://docs.docker.com/engine/storage/bind-mounts/[volumena] u dockeru.

Spajamo se na master server(``docker exec -it master bash``) i pravimo usera s grantovima potrebnim za replikaciju:

[source, mysql]
----
mysql> CREATE USER 'replica_user'@'%' IDENTIFIED WITH mysql_native_password BY 'password';
Query OK, 0 rows affected (0.02 sec)
mysql> GRANT REPLICATION SLAVE ON *.* TO 'replica_user'@'%';
Query OK, 0 rows affected (0.01 sec)
mysql> FLUSH PRIVILEGES;
Query OK, 0 rows affected (0.01 sec)

Unesemo nešto podataka i uradimo backup s mysqldump na masteru i restore na slaveu.

``mysqldump -u root -p --all-databases > backup.sql``

Kopiramo backup iz master kontenjera u slave.

``docker cp master:/backup.sql .`` ``docker cp ./backup.sql slave:/tmp``

Uradimo restore i provjerimo jesu li podaci tu.

mysql -u root -p < backup.sql
----

Spojimo se na mastera opet i nađemo poziciju binlogova:

[source, mysql]
----
mysql> show master status\G
*************************** 1. row ***************************
             File: mysql-bin.000012
         Position: 2400
     Binlog_Do_DB: 
 Binlog_Ignore_DB: 

Onda idemo na slave server i kucamo:
[source, mysql]
CHANGE MASTER TO
  MASTER_HOST = 'master',
  MASTER_USER = 'replica_user',
  MASTER_PASSWORD = 'password',
  MASTER_LOG_FILE = 'mysql-bin.000012',
  MASTER_LOG_POS = 2400;
----

[source, mysql]
----
mysql> start slave;
Query OK, 0 rows affected, 1 warning (0.05 sec)
mysql> show slave status\G
*************************** 1. row ***************************
               Slave_IO_State: Waiting for source to send event
                  Master_Host: master
                  Master_User: replica_user
                  Master_Port: 3306
                Connect_Retry: 60
              Master_Log_File: mysql-bin.000009
          Read_Master_Log_Pos: 923
               Relay_Log_File: mysql-relay-bin.000002
                Relay_Log_Pos: 1012
        Relay_Master_Log_File: mysql-bin.000009
             Slave_IO_Running: Yes
            Slave_SQL_Running: Yes...
----

U outputu za slave status vidimo da oba threada rade.
Možemo testirati da ubacimo nešto podataka na master i vidimo hoće li se replicirali na slavea.
U idućim vježbama susretati ćemo se još sa asinkronom replikacijom pa smo neke stvari ovdje izostavili.




== 2. Percona sinkrona replikacija

* link:https://docs.percona.com/percona-xtradb-cluster/8.4/docker-compose.html#directory-structure[Xtradb cluster docs]

Tu ima detaljan vodič kako dignuti cluster, al' sam ja na ovome primjeru učio šta rade varijable koje možemo koristiti u cnf fileovima.
Primjer cnf filea za node 1 kojega ćemo koristiti za ovaj cluster:


*server-id=1*: Jedinstveni identifikator za svaki MySQL poslužitelj u replikacijskom nizu.

*datadir=/var/lib/mysql*: Mapa gdje se nalaze sve baze podataka i tablice.

*log-error=/var/log/mysql/error.log*: Putanja do datoteke s bilješkama o pogreškama i upozorenjima.

*log_slave_updates=1*: Omogućuje da se promjene primljene s drugih servera bilježe u njegov binarni log. To je ključno za postavljanje replikacije u lancu.

*pxc-encrypt-cluster-traffic=OFF*: Isključuje enkripciju prometa između čvorova klastera.

*pxc_strict_mode=PERMISSIVE*: Dopušta korištenje eksperimentalnih varijabli, ali bilježi upozorenja u logove.

*bind-address=0.0.0.0*: Govori MySQL-u da sluša veze na svim mrežnim sučeljima. Umjesto *, koristi se 0.0.0.0.

*binlog_format=ROW*: Osigurava stabilnu Galera sinkronu replikaciju bilježenjem stvarnih promjena redaka u binarni log.

*gtid_mode=ON*: Uključuje Global Transaction ID za svaku transakciju, što pojednostavljuje replikaciju i oporavak.

*enforce_gtid_consistency=ON*: Strogo provodi GTID dosljednost, osiguravajući da su sve transakcije sigurne za GTID replikaciju.

*binlog_expire_logs_seconds=604800*: Automatski briše binarne logove starije od 7 dana.



*wsrep_provider=/usr/lib64/galera4/libgalera_smm.so*: Putanja do biblioteke koja implementira Galera replikacijski protokol.

*wsrep_cluster_address=gcomm://*: Adrese članova klastera. Ako je prazno (gcomm://), znači da je ovo prvi čvor (prvi koji se pokreće) i on uspostavlja klaster.

*wsrep_slave_threads=2*: Broj niti koje se koriste za primjenu promjena na čvoru, što ubrzava sinkronizaciju klastera.

*wsrep_log_conflicts=ON*: U logove bilježi konflikte koji nastaju u transakcijama.

*wsrep_node_address=node1*: IP adresa ili ime hosta ovog čvora.

*wsrep_cluster_name=pxc-cluster*: Ime klastera. Svi čvorovi s istim imenom čine klaster.

*wsrep_node_name=node1*: Jedinstveno ime čvora, koje se koristi za lakšu identifikaciju u logovima.

*wsrep_sst_method=xtrabackup-v2*: Metoda kojom se radi State Snapshot Transfer (SST). To je proces sinkronizacije novog čvora. xtrabackup-v2 je najčešća metoda.

*wsrep_provider_options="gcache.size=1G;gcs.fc_limit=10;gcs.fc_factor=0.8;"*: Varijable specifične za Galera replikaciju.Služe za kontrolu replikacije i stabilnost klustera.



*innodb_buffer_pool_size=1G*: Jedna od najvažnijih varijabli za performanse. Određuje veličinu memorije za keširanje podataka i indeksa.

*innodb_log_buffer_size=8M*: Određuje veličinu privremenog spremnika prije nego što se logovi transakcija sinkroniziraju na disk.

*innodb_log_file_size=128M*: Veličina pojedinačne datoteke redo loga.

*innodb_autoinc_lock_mode=2*: Način zaključavanja za AUTO_INCREMENT stupce. Vrijednost 2 omogućuje veću konkurentnost.

*innodb_file_per_table=1*: Svaka tablica ima svoju .ibd datoteku, što olakšava upravljanje i oslobađanje prostora.

*innodb_flush_log_at_trx_commit=2*: Određuje kada se logovi transakcija zapisuju na disk. Vrijednost 2 omogućuje da izgubimo podatke u rasponu od jedne sekunde, ali značajno poboljšava performanse.

*innodb_flush_method=O_DIRECT*: Metoda kojom se logovi zapisuju direktno na disk, zaobilazeći OS cache, što poboljšava I/O performanse.


Teorijsko objašnjavanje svake varijable je korisno, ali prava istina je da se Galera i MySQL konfiguracijske varijable najbolje uče kroz praksu.

Razne situacije zahtijevaju različite konfiguracije, a najbolje je testirati kako se klaster ponaša nakon svake promjene. Eksperimentiranje u kontroliranom okruženju, poput Docker kontejnera, omogućuje ti da sigurno istražuješ kako svaka varijabla utječe na performanse i stabilnost.

Svakako preporučujem da nastaviš istraživati i testirati druge varijable, posebno one unutar wsrep_provider_options, jer one imaju najveći utjecaj na ponašanje Galera klastera. Vremenom ćeš steći duboko razumijevanje koje ti ni jedna teorija ne može pružiti.

=== 2.0.1 Bootstrapanje noda

Bootstrapanje je proces pokretanja Galera klastera od nule. To je prva i najvažnija akcija koja se radi kad se želi dignuti klaster, bilo nakon prvog postavljanja, bilo nakon katastrofe (poput vraćanja iz backupa).

Kada se bootstrapa, jedan čvor (node) se pokreće kao "izvor istine". On je privremeni, jedini aktivni član klastera i označava početnu točku za sve replikacije. Ostali čvorovi će se spajati na njega, preuzimati njegovo stanje, i tek tada se klaster smatra funkcionalnim.

grastate.dat je datoteka koju Galera koristi za pohranu informacija o stanju čvora. Galera prati tri ključna podatka:

*UUID* (Unique Universal ID): Jedinstveni identifikator klastera.

*Seqno* (Sequence Number): Redni broj zadnje transakcije koju je taj čvor primio.

*safe_to_bootstrap*: 0 znači da ovaj čvor nije siguran za pokretanje novog klastera. 1 znači da se može bootsrapati s ovog nodea.

Galera koristi ove informacije pri pokretanju čvora da bi odlučila što treba učiniti.

Ako je UUID i Seqno isti kao na drugim čvorovima u klasteru, on se jednostavno spaja i replicira promjene.

Ako se razlikuje, on ulazi u proces State Snapshot Transfer (SST). To je proces u kojem traži od drugog čvora da mu pošalje kompletnu kopiju svih podataka, što se i događa u tvom restore scenariju.



=== 2.1 Testiranje pomoću sysbencha

U docker-compose.yaml dodati servis za sysbench:

[source, yaml]
----
tools:
    image: debian:12
    container_name: sysbenchtool2
    command: >
      bash -c "apt-get update && apt-get install -y sysbench && sleep infinity"
----

Na jednom nodu pravimo usera i bazu za testiranje:

[source, sql]
----
mysql> CREATE USER 'sbtest'@'%' IDENTIFIED BY 'pamet';
       GRANT ALL ON sbtest.* TO 'sbtest'@'%';
       CREATE SCHEMA sbtest;
----

Dignemo servis tools sa sysbenchom i počnemo testiranje:

[source, bash]
----
sysbench /usr/share/sysbench/oltp_write_only.lua --mysql-host=node1 --mysql-port=3306 --mysql-user=sbtest --mysql-password=pamet --mysql-db=sbtest --table_size=100000 --tables=10 --threads=25 --histogram=on --time=30 prepare

sysbench /usr/share/sysbench/oltp_write_only.lua --mysql-host=node1 --mysql-port=3306 --mysql-user=sbtest --mysql-password=pamet --mysql-db=sbtest --table_size=100000 --tables=10 --threads=25 --histogram=on --time=30 run

sysbench /usr/share/sysbench/oltp_write_only.lua --mysql-host=node1 --mysql-port=3306 --mysql-user=sbtest --mysql-password=pamet --mysql-db=sbtest --table_size=100000 --tables=10 --threads=25 --histogram=on --time=30 cleanup
----

Prepare unosi podatke u tablice, run odrađiva transakcije a cleanup čisti sve iza.

=== 2.2 Dodavanje asinkronog nodea preko GTID

Dodamo servis u docker-compose.yaml za asinkroni node:

[source, yaml]
----
  areplica:
    image: percona/percona-xtradb-cluster:8.0.41
    container_name: areplica
    deploy:
      resources:
        limits:
          cpus: '1'
          memory: 4G
    volumes:
      -  ./areplica_data:/var/lib/mysql
      -  ./config/areplica.cnf:/etc/mysql/conf.d/my.cnf
      -  ./logar:/var/log/mysql
    environment:
      MYSQL_ROOT_PASSWORD: 4GwihtremRDcQF
----

U konfiguracijski file za asinkroni node stavimo:

[source, cnf]
----
[mysqld]

server-id=4
bind_address=0.0.0.0
datadir=/var/lib/mysql
relay-log=/var/log/mysql/mysql-relay-bin.log
log-error=/var/log/mysql/error.log
binlog_format=ROW 
binlog_expire_logs_seconds=604800 
gtid_mode=ON
enforce_gtid_consistency=ON 
log_slave_updates=1
innodb_buffer_pool_size=1G
innodb_log_buffer_size=8M
innodb_log_file_size=128M
innodb_autoinc_lock_mode=2
innodb_file_per_table=1
innodb_flush_log_at_trx_commit=1
innodb_flush_method=O_DIRECT
read_only=ON
----

Dignemo asinkroni node i onda radimo fizički backup i restore pomoću xtrabackupa.

Za svaki slučaj obrišemo datu asinkronom nodu da restore može biti čist.

``sudo rm -rf asinkrona_data/*``

Na masteru radimo backup:

/usr/bin/pxc_extra/pxb-8.0/bin/xtrabackup --backup --target-dir=/tmp/backup --user=root --password=4GwihtremRDcQF --slave-info``

Možemo sherati bindani folder tako da ne moramo prebacivati backup, samo u volumes nodu gdje radimo backup i asinkronoj replici dodamo ovu liniju:

``-  ./backups:/tmp/backups``

Radimo restore pomoću komandi:

``/usr/bin/pxc_extra/pxb-8.0/bin/xtrabackup --prepare --target-dir=/tmp/backups``

``/usr/bin/pxc_extra/pxb-8.0/bin/xtrabackup --copy-back --target-dir=/tmp/backups``

Restoranoj dati moramo dati opet permisije:

``sudo chown -R 1001:1001 areplica_data``

Na nekom master nodu napravimo replika usera i onda na slaveu kucamo:

[source, mysql]
----
CHANGE MASTER TO
  MASTER_HOST = 'node1',
  MASTER_PORT = 3306,
  MASTER_USER = 'replica_user',
  MASTER_PASSWORD = 'password',
  SOURCE_DELAY=60,
  MASTER_AUTO_POSITION = 1;

start slave;
show slave status\G
----

Source_delay daje replikacijski delay od 60s. Što je korisno u nekim situacijama.

[TIP]
====
U nekim situacijama korisna je komanda, služi za preskakanje ako neka transakcija zapne, al se mora koristiti s oprezom pošto utječe na integritet podataka.

``STOP REPLICA;

SET GLOBAL SQL_SLAVE_SKIP_COUNTER = 1; 

START REPLICA;``
====



=== 2.3 Dodavanje Haproxya ispred instanci

U postojeći setup u docker-compose.yaml dodajemo image za haproxy:

[source, yaml]
----
haproxy:
    container_name: haproxy
    image: haproxy:3.2
    deploy:
      resources:
        limits:
          cpus: '1'
          memory: 1G
    volumes:
      -  ./config/haproxy.cfg:/usr/local/etc/haproxy/haproxy.cfg
    logging:
        driver: "local"
        options:
            max-size: "10m"
            max-file: "3"
----

I pravimo cfg file na gore pomenutoj lokaciji gdje ubacivamo:

[source, cfg]
----
global
    log stdout local0
    maxconn 4096
    daemon

defaults
    log global
    mode tcp
    option tcplog
    option dontlognull
    option log-health-checks
    option asap
    timeout connect 5000
    timeout client 50000
    timeout server 50000
    retries 3

frontend write
    bind *:3306
    mode tcp
    option clitcpka
    default_backend mysql_backend_write

frontend read
    bind *:3307
    mode tcp
    option clitcpka
    default_backend mysql_backend_read

backend mysql_backend_write
    mode tcp
    option srvtcpka
    balance leastconn
    server node1 node1:3306 check inter 2s rise 2 fall 3 weight 1
    server node2 node2:3306 check inter 2s rise 2 fall 3 weight 1 backup
    server node3 node3:3306 check inter 2s rise 2 fall 3 weight 1 backup

backend mysql_backend_read
    mode tcp
    option srvtcpka
    balance leastconn
    server node1 node1:3306 check inter 2s rise 2 fall 3 weight 1 backup
    server node2 node2:3306 check inter 2s rise 2 fall 3 weight 1 
    server node3 node3:3306 check inter 2s rise 2 fall 3 weight 1

----

TIP: Ako se konfiguracija piše u VSCodeu može se desiti da ne prođe jer zadnji red treba biti prazan da zna kada je kraj konfiguracije, pa pomoću nekog drugog txt editora dodamo prazan zadnji red pošto to ne očita u VSCodeu.


Ova konfiguracija omogućuje da writeovi uvijek idu na jedan node a readovi na druga dva.

Može se sa sysbench alatom testirati radi li.

=== 2.4 Upotreba Percona tool kita

Prvo ubacimo sliku percona tool kita u yaml file:
[source, yaml]
----
perconatool:
    image: perconalab/percona-toolkit:latest
    container_name: ptoolkit
    command: tail -F /dev/null   # s ovom komandom kontenjer stalno ostaje živ da mi možemo vježbati i istraživati
----


*pt-archiver*

Služi za arhiviranje podataka. Najveća mu je prednost što može raditi na 'živim' tablicama.
Vježbamo na istom percona setupu kao source a dignuti cemo mariadb cluster koji ce nam poslužiti kao destination. Pošto imamo samo sbtest bazu prebaciti ćemo jedan dio neke tablice u arhivu.
Možemo uzeti sbtest5 tablicu i prebaciti sve podatke koji imaju id manji od 2000 u arhivsku bazu.

Na source serveru kucamo:
``show create table sbtest5\G``

Na destination serveru pravimo bazu naziva **arhiva** i kopiramo create table što smo dobili iz prošle komande.

[source, mysql]
----
MariaDB [arhiva]> CREATE TABLE `arhiva_sbtest5` (
    ->   `id` int NOT NULL AUTO_INCREMENT,
    ->   `k` int NOT NULL DEFAULT '0',
    ->   `c` char(120) NOT NULL DEFAULT '',
    ->   `pad` char(60) NOT NULL DEFAULT '',
    ->   PRIMARY KEY (`id`),
    ->   KEY `k_5` (`k`)
    -> ) ENGINE=InnoDB AUTO_INCREMENT=300001 DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_0900_ai_ci;
Query OK, 0 rows affected (0.030 sec)
----

Na source smo provjerili i koliko imamo tih podataka da možemo potvrditi prebacivanje.

[source, mysql]
----
mysql> SELECT COUNT(*) FROM sbtest5 WHERE id < 2000;
+----------+
| COUNT(*) |
+----------+
|      667 |
+----------+
----

Ulazimo u kontenjer gdje nam se nalazi percona tool kit i istraživamo koje nam sve opcije trebaju da prebacimo podatke.

``bash-5.1$ pt-archiver --source 'h=node1,u=root,p=4GwihtremRDcQF,P=3306,D=sbtest,t=sbtest5' --where 'id < 2000' --dest 'h=172.22.0.2,u=root,p=JNC8D6HjtusQ5i,P=3306,D=arhiva,t=arhiva_sbtest5' --limit=30 --dry-run --statistics --no-check-columns --no-delete --retries=5 --sleep=1``

TIP: Uvijek prvo probamo s dry run, gdje će komanda proći sve korake ali neće izvršiti komandu.

[source, bash]
----
pt-archiver \
    --source 'h=node1,u=root,p=4GwihtremRDcQF,P=3306,D=sbtest,t=sbtest5' \
    --where 'id < 2000' \
    --dest 'h=172.22.0.2,u=root,p=JNC8D6HjtusQ5i,P=3306,D=arhiva,t=arhiva_sbtest5' \
    --limit=30 \
    --statistics \
    --no-check-columns \
    --retries=5 \
    --sleep=1
----

[source, mysql]
----
Started at 2025-09-12T10:21:45, ended at 2025-09-12T10:22:11
Source: D=sbtest,P=3306,h=node1,p=...,t=sbtest5,u=root
Dest:   D=arhiva,P=3306,h=172.22.0.2,p=...,t=arhiva_sbtest5,u=root
SELECT 667
INSERT 667
DELETE 667
Action         Count       Time        Pct
sleep             23    23.0064      85.48
commit          1336     3.0007      11.15
deleting         667     0.3710       1.38
inserting        667     0.3458       1.28
select            24     0.0322       0.12
other              0     0.1581       0.59
----

Možemo ući u mariadb da vidimo jesu tu podaci.

[source, mysql]
----
MariaDB [arhiva]> select count(*) from arhiva_sbtest5;
+----------+
| count(*) |
+----------+
|      667 |
+----------+
----

*pt-online-schema-change*

Ovaj alat služi za promjenu sheme tablice bez zaključavanja tablice i bez ikakvog zastoja za bazu. Idealan je za produkcijske sustave gdje se ne smije prekidati rad aplikacija.

U vježbi ćemo dodati novi stupac created_at u arhivsku tablicu, kako bismo znali kada su podaci prebačeni u arhivu.

Prvo, uđi u svoj MariaDB kontejner i provjeri trenutnu shemu tablice arhiva_sbtest5.
To je važno da se uvjeriš da stupac ne postoji i da ispravno ciljaš tablicu.

``MariaDB [arhiva]> show create table arhiva_sbtest5\G``

Sada, unutar ptoolkit kontejnera, pokreni pt-online-schema-change. Koristit ćemo --dry-run da vidimo hoće li sve proći bez problema, ali bez stvarne izmjene tablice.

[source, bash]
----
pt-online-schema-change \
    --alter "ADD COLUMN created_at TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP AFTER pad" \
    --host=maria1 \
    --user=root \
    --password=JNC8D6HjtusQ5i \
    --port=3306 \
    --database=arhiva \
    --table=arhiva_sbtest5 \
    --max-load Threads_running=50,Threads_connected=100 \
    --chunk-size=1000 \
    --check-interval=1 \
    --recursion-method=NONE \
    --dry-run
----

Nakon što se uvjeriš da je dry-run prošao bez greške, možeš pokrenuti pravu promjenu. Jednostavno ukloni --dry-run i dodaj opciju --execute.

[source, mysql]
----
MariaDB [arhiva]> show create table arhiva_sbtest5\G
*************************** 1. row ***************************
       Table: arhiva_sbtest5
Create Table: CREATE TABLE `arhiva_sbtest5` (
  `id` int(11) NOT NULL AUTO_INCREMENT,
  `k` int(11) NOT NULL DEFAULT 0,
  `c` char(120) NOT NULL DEFAULT '',
  `pad` char(60) NOT NULL DEFAULT '',
  `created_at` timestamp NOT NULL DEFAULT current_timestamp(),
  PRIMARY KEY (`id`),
  KEY `k_5` (`k`)
) ENGINE=InnoDB AUTO_INCREMENT=300001 DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_0900_ai_ci
1 row in set (0.001 sec)
----

Možemo ga koristiti i za optimiaziciju tablica, evo jednog primjera:

Nakon dugo testiranja jedne tablice, pisanja i brisanja njezina veličina je bila ovakva:

[source, mysql]
----
root@n01:/# dbsize
+--------------------+-----------+
| Database           | Size (MB) |
+--------------------+-----------+
| information_schema |      0.20 |
| mysql              |      3.43 |
| performance_schema |      0.00 |
| sbtest             |     66.05 |
| sys                |      0.03 |
| vrijeme            |      0.03 |
+--------------------+-----------+

root@n01:/# du -h /var/lib/mysql/

4.0M	/var/lib/mysql/mysql
4.0K	/var/lib/mysql/performance_schema
608K	/var/lib/mysql/sys
168K	/var/lib/mysql/vrijeme
1.9G	/var/lib/mysql/sbtest
13G	/var/lib/mysql/
root@n01:/# dbsize
----

Pustili smo pt-osc da odradi svoje:

[source, bash]
----
bash-5.1$ pt-online-schema-change --alter "ENGINE=InnoDB" D=sbtest,t=sbtest1 --host=10.200.0.36 --port=3306 --user=root --password=psici --recursion-method=none --chunk-size=1000 --check-interval=1 --execute
----

I poslije vidimo:

[source, mysql]
----
root@n01:/# dbsize
+--------------------+-----------+
| Database           | Size (MB) |
+--------------------+-----------+
| information_schema |      0.20 |
| mysql              |      3.43 |
| performance_schema |      0.00 |
| sbtest             |     30.06 |
| sys                |      0.03 |
| vrijeme            |      0.03 |
+--------------------+-----------+

root@n01:/# du -h /var/lib/mysql/

4.0M	/var/lib/mysql/mysql
4.0K	/var/lib/mysql/performance_schema
608K	/var/lib/mysql/sys
168K	/var/lib/mysql/vrijeme
41M	/var/lib/mysql/sbtest
11G	/var/lib/mysql/
----




=== 2.5 Indexi u MySQLu







== 3. MariaDB sinkrona replikacija

Dignuti cluster s mariadb node-a.Jedina razlika od Percona clustera je što u cnf fileu moramo dodati usera za sst backup ``wsrep_sst_auth=root:pass``  pa idemo odmah na iduću vježbu.

=== 3.1 Mariadb full fizički backup i restore

==== backup

* link:https://mariadb.com/docs/server/server-management/install-and-upgrade-mariadb/installing-mariadb/binary-packages/automated-mariadb-deployment-and-administration/docker-and-mariadb/container-backup-and-restoration[Maria-backup docs]

Odvajanje backup procesa u zasebnu komponentu (kontejner) omogućuje standardizaciju. Bez obzira na vrstu baze podataka (MySQL, MariaDB, MongoDB, itd.), backup kontejner uvijek radi na isti način. Spoji se na bazu, povuče podatke, i spremi ih na željenu lokaciju. To znatno pojednostavljuje automatizaciju i upravljanje infrastrukturom.

Iako se može činiti kao dodatni korak, korištenje zasebnog kontejnera za backup dugoročno smanjuje složenost, povećava sigurnost i stabilnost produkcijskog sustava. Zato je to best practice u modernim infrastrukturama.

Dignemo backup kontenjer, a yaml za njega smo stavili:
[source, yaml]
----
mariabackup:   # pravimo servis za backup
    image: mariadb:11.4   # vucem citav image mariadb posto ce imati toolse, a nisam mogao naci sliku samo s toolsima 
    container_name: mariabackup   # dajemo ime kontenjeru
    deploy:                       # ogranicivamo resurse
      resources:
        limits:
          cpus: '1'
          memory: 4G
    volumes:                     # mountamo volume od node2 u kontenjer za backup i volume gdje cemo cuvati backupe
      -  ./maria2_datanew:/var/lib/mysql:ro
      -  ./backups2:/var/backups
    command: ["sleep", "infinity"]   # kazem kontenjeru ako mu se ukine glavni proces(mariadb) da ipak ostane u njemu
----

Napravimo na nekom nodu usera za backup s potrebnim minimalnim privilegijama:

[source, mysql]
----
CREATE USER 'mariabackup'@'%'
 IDENTIFIED BY 'mbu_passwd';

 GRANT RELOAD, PROCESS, LOCK TABLES, REPLICATION CLIENT
 ON *.* TO 'mariabackup'@'%';
----

Uđemo u backup kontenjer i kucamo komandu:

``mariadb-backup --backup --host=maria2 -u mariabackup -p mbu_passwd --target-dir=/var/backups``

Sada imamo backup u folderu na hostu, jer smo bindali target dir.

Ovo je najjednostavniji oblik backupa, backupe još možemo npr. streamati i kompresirati u jedan file i slati u neki bucket, što može biti neka druga vježba u kombinaciji sa skriptiranjem backupa i stavljanjem crontabova za redovno okidanje.

==== restore

U praksi, restore najčešće koristimo u dva scenarija:

1. Testiranje ispravnosti backupa: Redovito vraćanje baze podataka na testnom poslužitelju jedini je siguran način da se provjeri radi li proces backupa ispravno. To je kritičan korak u osiguravanju da, u slučaju prave katastrofe, naša sigurnosna kopija zaista funkcionira i da su podaci pouzdani.

2. Stvarni oporavak od katastrofe (DR): Ako se dogodi najgore, ovaj proces nam omogućuje da vratimo sustav u operativno stanje koristeći posljednju ispravnu kopiju podataka.

Dakle, iako je restore operacija koju ne želimo izvoditi, njezino testiranje je obavezno. Dobra praksa je da se to radi redovito, čime se stječe sigurnost u ispravnost našeg procesa i spremnost za svaki scenarij.

Simuliramo katastrofu i spustimo sva 3 nodea.

Dignemo kontenjer za backup i restore i u njemo odmah mozemo restorati podatke pošto je data folder bindan za node na kojem ćemo raditi bootstrap u ovom slučaju node naziva maria2.

Prvo radimo prepare pa kopiranje fileova:

``mariadb-backup --prepare --target-dir=/var/backups``

``mariadb-backup --copy-back --target-dir=/var/backups``

Dajemo novoj dati prava da mariadb proces ima prava koristiti i čitati novu datu ``sudo chown -R 1001:1001 maria2_datanew`` 

Sada pripremimo node koji je dijelio datu za bootstrap. To jest da pravi novi cluster koji će ostalima biti novi izvor istine.

Komandu za boostrap u yaml fileu stavljamo na njega. A ostalima provjerimo da su ``Safe to bootstrap: 0`` u grastate datoteci.

Dižemo i druga dva noda koji će se pomoću SST spojiti i formirati novi cluster od 3 nodea.



=== 3.2 Mariadb incremental fizički backup i restore

Glavni razlog zašto koristimo inkrementalne backupe je učinkovitost. Dok je puni backup (full backup) ključna polazna točka, njegovo svakodnevno ponavljanje može biti vrlo skupo u smislu vremena i prostora na disku. Zamisli da imaš bazu podataka od 500 GB. Svaki dan raditi novi, puni backup znači da ti treba dodatnih 500 GB prostora i sat vremena ili više da se završi. To jednostavno nije održivo.

Tu na scenu stupa inkrementalni backup. On kopira samo one promjene (novi redovi, ažuriranja, brisanja) koje su se dogodile od posljednjeg backupa, bez obzira je li taj bio puni ili inkrementalni. Ova metoda štedi:

Prostor na disku: Umjesto gigabajta, inkrementalni backup može biti samo nekoliko megabajta ili gigabajta, ovisno o aktivnosti baze.

Vrijeme: Proces je mnogo brži jer ima manje podataka za kopiranje.


Incremental backup ne mozemo raditi bez full backupa a imamo ga iz prijasnje vjezbe pa sad unesemo nesto novih podataka da imamo sta backupirati s incremental backupom.

Incremental backup opet radimo iz backup kontenjera:

``mariadb-backup --backup --target-dir=/var/backups/noviinc --incremental-basedir=/var/backups2 --host=maria2 --user=mariabackup --password=mbu_passwd``

target dir je direktorij gdje će biti sačuvan incr backup a base dir je direktorij od full backupa.

Sada opet spustamo sva 3 noda i brisemo im datu iz bindanih volumena.

Onda radimo prepare backupa, prvo full pa incremental:

``mariadb-backup --prepare --target-dir=/var/backups/novi``

``mariadb-backup --prepare --target-dir=/var/backups/novi --incremental-dir=/var/backups/noviinc``

TIP: Kad imamo više incrementalnih backupa tipa ponedjeljak, utorak, srijeda, četvrtak... Prvo prepare ponedjeljak pa utorak pa srijeda pa četvrtak...

Pa radimo restore:

``mariadb-backup --copy-back --target-dir=/var/backups/novi``

Dizemo node2 s novim izvorom istine i spajamo druga dva noda.

=== 3.3 Mariadb logički backup s Mydumperom



=== 3.4 Mariadb PITR(Point in time restore) proces

=== 3.5 Vježbanje skripti za backupe

=== 3.6 Postavljanje Maxscale proxya ispred clustera

U docker-compose.yaml dodajemo image od maxscalea:

[source, yaml]
----
maxscale:
    image: mariadb/maxscale:24.02
    container_name: maxscale
    deploy:
      resources:                       
        limits:
          cpus: '2'
          memory: 4G
    volumes:                                            
      -  ./config/maxscale.cnf:/etc/maxscale.cnf
      -  ./logmax:/var/log/maxscale
      -  ./maxdata:/var/lib/maxscale
----

U cnf file ./config/maxscale.cnf ubacujemo:

[source, cfg]
----
[maxscale]
threads=2
log_augmentation=1
ms_timestamp=1
logdir=/var/log/maxscale

[server1]
type=server
address=mariam1
port=3306
protocol=MariaDBBackend

[server2]
type=server
address=mariam2
port=3306
protocol=MariaDBBackend

[server3]             
type=server
address=mariam3
port=3306
protocol=MariaDBBackend

[ar_server]
type=server
address=amariam4
port=3306
protocol=MariaDBBackend

[Galera-Monitor]
type=monitor
module=galeramon
servers=server1,server2,server3
user=maxscale
password=rNtOdlAg30SkaF
monitor_interval=2000ms

[Async-Slave-Monitor]
type=monitor
module=mariadbmon
servers=ar_server
user=maxscale
password=rNtOdlAg30SkaF
monitor_interval=2000ms

[Read-Write-Service]
type=service
router=readwritesplit
servers=server1,server2,server3
user=maxscale
password=rNtOdlAg30SkaF
master_failure_mode=fail_on_write
connection_keepalive=300s          

[Read-Write-Listener]
type=listener
service=Read-Write-Service
protocol=MariaDBClient
port=4306

[RO-Service]
type=service
router=readconnroute
servers=server1,server2,server3
router_options=slave
user=maxscale
password=rNtOdlAg30SkaF

[RO-Listener]
type=listener
service=RO-Service
protocol=MariaDBClient
port=4010
----

Ova MaxScale konfiguracija je postavljena da upravlja prometom u klasteru baze podataka i usmjerava upite na odgovarajuće servere.

Globalne postavke za maxscale
Ovo su osnovne globalne postavke, kao što su broj niti (threads=2) i lokacija logova (logdir=/var/log/maxscale).

Serveri
Definira četiri fizička servera baze podataka (mariam1, mariam2, mariam3, amariam4) s njihovim adresama i portovima.

Monitori
Dva monitora prate status i topologiju servera.

Galera-Monitor prati Galera klaster (serveri 1-3).

Async-Slave-Monitor prati asinhronu repliku (ar_server).

Servisi
Ovo su logike za usmjeravanje prometa.

Read-Write-Service koristi readwritesplit ruter. On automatski razdvaja upite za čitanje od upita za pisanje i šalje ih na odgovarajuće servere (master/slave).

RO-Service koristi readconnroute ruter s opcijom slave. Ovaj servis je namijenjen isključivo za upite za čitanje, koje usmjerava samo na slave servere radi balansiranja opterećenja.

Listeneri
Listeneri su ulazne tačke za klijente.

Read-Write-Listener na portu 4306 usmjerava promet na Read-Write-Service.

RO-Listener na portu 4010 usmjerava promet na RO-Service, omogućavajući samo operacije čitanja.

Ukratko, konfiguracija pruža dva različita načina pristupa klasteru: jedan s punim pravima (čitanje/pisanje) i jedan isključivo za čitanje.

Na jednom nodu pravimo usera za maxscale da ima pristup:

[source, mysql]
----
CREATE USER 'maxscale'@'%' IDENTIFIED BY 'rNtOdlAg30SkaF';
 GRANT SELECT ON mysql.user TO 'maxscale'@'%';
 GRANT SELECT ON mysql.db TO 'maxscale'@'%';
 GRANT SELECT ON mysql.tables_priv TO 'maxscale'@'%';
 GRANT SELECT ON mysql.columns_priv TO 'maxscale'@'%';
 GRANT SELECT ON mysql.procs_priv TO 'maxscale'@'%';
 GRANT SELECT ON mysql.proxies_priv TO 'maxscale'@'%';
 GRANT SELECT ON mysql.roles_mapping TO 'maxscale'@'%';
 GRANT SHOW DATABASES ON *.* TO 'maxscale'@'%';
 GRANT RELOAD ON *.* TO 'maxscale'@'%';
 GRANT PROCESS ON *.* TO 'maxscale'@'%';
 GRANT SLAVE MONITOR ON *.* TO 'maxscale'@'%';
 GRANT REPLICATION CLIENT ON *.* TO 'maxscale'@'%';
 GRANT REPLICATION SLAVE ON *.* TO 'maxscale'@'%';
 FLUSH PRIVILEGES;
----

Dižemo maxscale, ulazimo u kontenjer i tu možemo vidjeti servere s komandom:

``maxctrl list servers``

Već spojenu asinkronu repliku koja ide preko GTID možemo joj sad prebaciti konekciju da ide preko maxscale i to RO-Listenera.
Na asinkronoj replici kucamo:

[source, mysql]
----
stop slave;
CHANGE MASTER TO 
 MASTER_HOST='maxscale', 
 MASTER_PORT=4010, 
 MASTER_USER='maxscale', 
 MASTER_PASSWORD='rNtOdlAg30SkaF',
 MASTER_USE_GTID=slave_pos,
 MASTER_SSL=0;
start slave;
----

[source, mysql]
----
MariaDB [(none)]> show slave status\G
*************************** 1. row ***************************
                Slave_IO_State: Waiting for master to send event
                   Master_Host: maxscale
                   Master_User: maxscale
                   Master_Port: 4010
                 Connect_Retry: 60
               Master_Log_File: master-bin.000045
           Read_Master_Log_Pos: 359
                Relay_Log_File: mysql-relay-bin.000002
                 Relay_Log_Pos: 659
         Relay_Master_Log_File: master-bin.000045
              Slave_IO_Running: Yes
             Slave_SQL_Running: Yes
----

=== 3.7 Dodavanje i proučavanje TLS certifikata za MariaDB instance

Kreiramo direktorij gdje cemo cuvati certifikate

 ``mkdir certs``

Uđemo u taj direktorij i kreiramo certifikate:

[source, bash]
----
 openssl genrsa 2048 > ca-key.pem

 openssl req -new -x509 -nodes -days 3650 -key ca-key.pem -out ca.pem -subj "/CN=MyMariaDBClusterCA/O=MyOrg/C=BA"

 openssl req -newkey rsa:2048 -days 3650 -nodes -keyout server-key.pem -out server-req.pem -subj "/CN=server.mycluster.local/O=MyOrg/C=BA"

 openssl x509 -req -in server-req.pem -days 3650 -CA ca.pem -CAkey ca-key.pem -set_serial 01 -out server-cert.pem

 openssl req -newkey rsa:2048 -days 3650 -nodes -keyout client-key.pem -out client-req.pem -subj "/CN=client.mycluster.local/O=MyOrg/C=BA"

 openssl x509 -req -in client-req.pem -days 3650 -CA ca.pem -CAkey ca-key.pem -set_serial 02 -out client-cert.pem

 rm *-req.pem  
----

Dobijemo 6 fileova, od kojih ca key je kljuc kojim pravimo ca.pem koji nam sluzi za opečatiti ostale certifikate kao npr serverskih i klijentskih.

Zbog jednostavnosti vježbe kopirali smo ih u jos jedan folder i jednome smo folderu dali permisije od mariadb a drugom od maxscalea.

Onda smo ih mountali u kontenjere s opcijom read only, maria u maria , max u max.

Prvo smo ih ubacili u nodove od galera clustera da se enkriptira promet izmedju replika. Galerin interni SSL nam kaze da mozemo staviti server certifikate da ne treba client.

Onda serverske certifikate stavljamo na svaki node a klijentske u cnf maxscale ispod svakog definiranog servera. Da pokrijemo komunikaciju izmedju maxscalea i naseg galera clustera.

Iduci korak je da serverske certifikate stavljamo na maxscale listenere da pokrijemo veze koje se spajaju na njih. Npr. kao veza asinkrone replikacije i maxscalea koju onda moramo uspostaviti sa:

[source, mysql]
----
 CHANGE MASTER TO 
 MASTER_HOST='maxscale', 
 MASTER_PORT=4010, 
 MASTER_USER='maxscale', 
 MASTER_PASSWORD='rNtOdlAg30SkaF',
 MASTER_USE_GTID=slave_pos;
 MASTER_SSL=1,
 MASTER_SSL_CERT='/etc/mysql/certs/client-cert.pem',
 MASTER_SSL_KEY='/etc/mysql/certs/client-key.pem',
 MASTER_SSL_CA='/etc/mysql/certs/ca.pem',
 MASTER_SSL_VERIFY_SERVER_CERT=0;
----

Zadnja linija nam sluzi da iskljucimo verifikaciju prema serverima pošto nismo uključili ime servera u CN pri pravljenju certifikata pa ga nece prepoznati.

PS. treba i alter usera

 ``ALTER USER 'maxscale'@'%' REQUIRE SSL;``

=== 3.8 PMM monitoring i dashboardi

== 4. MongoDB replika set

Prvo pisemo docker-compose.yaml file

Pravimo foldere za conf fileove koje cemo mountati kroz yaml i dodajemo u njih sta nam treba --> mongo1.cnf mongo2.cnf mongo3.cnf (2 i 3 su isto kao 1)

Pravimo foldere za datu koju cemo mountati kroz yaml --> data1_mongo data2_mongo data3_mongo (mkdir) // on ce i sam napraiviti foldere al nece imati permisije zato ih ja odmah napravim i dadnem permisije.

Pravimo foldere za logove koje cemo mountati kroz yaml --> log1 log2 log3 (mkdir)

Dajemo im permisije za pisanje i vlasništvo nad data i log folderima da to mongo instanca može i raditi iz dockera (chown, chmod) \ davao sam za chmod 777 da mi sigurno radi al u praksi ne treba tako

Pošto smo u conf enable security moramo sada napraviti key file i mountati ga premo yaml file u kontenjer za svaki node /etc/mongodb-keyfile

 ``openssl rand -base64 756 > ./mongodb-keyfile``  \\ ne znam sada gdje ga drzimo ja sam ga ostavio u ovom folderu
 ``chmod 0400 /etc/mongo-keyfile``  \\ moramo mu dati permisije samo za citanje inace ga mongo instanca nece prihvatiti
 ``chown -R 999:999 /etc/mongo-keyfile``

Sve dobro provjeriti i vidjeti u yaml file jesu li imena i fileovi dobro mountani \ posebno zbog čestih typo

Pomoću docker-compose podignuti sve 3 instance(!obavezno jednu po jednu!)

Spojiti se na jednu i inicializirati replica set:

[source, mongodb]
 rs.initiate(
 {
 _id: "rs0",  // id replica seta
 version: 1,
 members: [
          { _id: 0, host : "mongors1:27017" },
          { _id: 1, host : "mongors2:27017" },
          { _id: 2, host : "mongors3:27017" }         ] } )


Upotrijebiti admin database i dodati usera:

[source, mongodb]
use admin
db.createUser( { user: "admin", pwd: "ass", roles: [{ role: "root", db: "admin" }] })

Opet se logirati i provjeriti status replica seta:

``rs.status()``

Automatsko mijenjanje primary nodea,upišemo na primarnom nodu:

``rs.stepDown(60)``

Manualno mijenjaje primary nodea,na primarnom nodu upišemo:

[source, mongodb]
cfg = rs.conf()   // Trenutna konfiguracija replica seta
cfg.members[2].priority = 2  //stavimo mu veci prioritet tako znamo da ce on biti izabran
rs.reconfig(cfg)  // primjenjuje novu konfiguraciju
rs.stepDown(60)  // automatski mu je 60 sekundi ali stavimo svakako

Sada ce replica izabrati node s _id 2 da bude primarni posto smo njemu dali veci prioritet.

=== 4.1 Dodavanje Hidden noda u replika set


Hidden nam služi da radimo backupe ili analitiku preko njega da ne bi opterećivali ostale nodove.

Prvo ga dodamo u docker-compose.yaml

Napravimo i za njega mongoh.conf koji cemo mountati kroz yaml

Napravimo foldere za datu i log fileove koje cemo mountati u yamlu s permisijama --> mongohidden_data loghidden \ dati permisije i vlasnistva

I njemu moramo mountati key file(procedura opisana u prijasnjoj vježbi).

Dignemo ga s docker-compose

Spojimo se na primary instancu i pridružimo ga replika setu:

[source, mongodb]
 rs.add({
 host: "mongohidden:27017",  // njegova adresa
 hidden: true,               // da je skriven
 priority: 0,                // da ne moze biti izabran za primarnog
 votes: 0,                   // da ne moze glasati
 secondaryDelaySecs: 3600    // da pise podatke s zaostatkom od 1h, ako se slucajno pobrise nesto s primarnog da mozemo brze vratiti podatke 
 })

Provjeriti status, njemu bi trebalo pisati **hidden: true**

=== 4.2 Dodavanje Arbiter noda u replika set

Arbiter nam sluzi u slucaju parnog broja nodova da bude dodatni glas kojim se moze izabrati primarni (najčešći slučaj je 2 noda + arbiter)

Prvo ga dodamo u docker-compose.yaml file.

Njemu ne treba folder za datu, mozemo napraviti samo za logove --> logarbiter // i dati mu permisije i vlasnistva

Napraviti i mongoarb.conf file za njega, mozemo komentirati sve vezano za podatke a ostaviti postavke za replikaciju, i mountati ga u yamlu.

I njemu se mora mountati key file.

Dizemo ga s docker-compose

Pridruzimo ga replica setu s primarnog noda

[source, mongodb]
 rs.addArb({"mongoarb:27017"}) 

Sada mozemo provjeriti stanje:

 ``rs.status()``


=== 4.3 Backup s mongodump

Pravimo novi kontenjer 'mongobackup' s istom slikom kao i nodovi na kojima ce raditi backup --> docker-compose.yaml.

Mountamo novi folder gdje cemo cuvati backupe --> backups.

U enviromentu dodamo novog usera za pravljenje backupa.

Dignemo novi kontenjer pomocu docker-compose i spojimo se na njega.

Moramo napraviti usera i na replici setu da se može raditi backup, tako da se spojimo na primarnu instancu i kreiramo ga:

[source, mongodb]
 db.createUser(
 {
 user: "BackupUser",  
 pwd: "123", 
 roles: [ { role: "backup", db: "admin" } ,
         { role: "restore", db: "admin" } ]
 }
 )

Na bash liniji u kontenjeru za backup kucamo:

 ``mongodump --host=mongors2:27017 --username=BackupUser --password=123  --authenticationDatabase admin -readPreference=secondary --oplog --gzip --archive=/var/backups``

I imamo dump svih baza.

=== 4.4 Restore s mongodump

Prvo spustimo sva tri noda posto 'emo raditi "novi replica set" 

Pobrišemo svu datu iz sva tri noda.

Dignemo jedan node i na njemu uradimo inicijalizciju i dodavanje usera.

[source, mongodb]

 rs.initiate(
 {
 _id: "rs0",
 members: [
         { _id: 0, host : "mongors1:27017" } ] } )

 use admin

 db.createUser(
 {
 user: "BackupUser",                                      
 pwd: "123", // 
 roles: [ { role: "backup", db: "admin" } ,
         { role: "restore", db: "admin" } ]
 }
 )     


Iz kontenjera 'mongobackup' na bash liniji kucamo:

``mongorestore --archive=/var/backups/mongodump-2025-05-26.archive --gzip --host=mongors1:27017 -u BackupUser -p 123 --authenticationDatabase=admin``

Provjerimo jesu li podaci tu na prvom nodu al prvo se logiramo sa nasim userom.

``show dbs``

Ako jesu dižemo i druga dva noda s docker-compose.

I dodajemo ih u replika set.

[source, mongodb]
 rs.add('mongors2:27017')
 rs.add('mongors3:27017')

 Sada bi trebali imati sve podatke opet.


=== 4.5 Restore PITR

Scenario: U bazi smo napravili danas neku novu kolekciju i još nešto radili, i sad slučajno pobrišemo neku staru kolekciju jer je bila sličnog imena.

Moramo imati backup s oplogom i on mora sadržavati zajedničku točku s novim oplogom,tj. scoop oploga mora uhvatiti i backupov.

Uradimo dump oploga iz backup kontenjera:

 ``mongodump --host=mongors2:27017 --username=BackupUser --password=123   --authenticationDatabase admin -d local -c oplog.rs -o oplogD``

Premjestimo oplog u novi direktorij:

 ``mv oplogD/local/oplog.rs.bson oplogR/oplog.bson``

Sada mozemo procitati oplog file pomocu bsondump ili naći u bazi ako još imamo timestamp kada je dropana kolekcija.

Pošto u mene jos je živa mongo instanca provjeravam pomoću:
[source, mongodb]
 db.oplog.rs.find({
 "op": "c",
 "o.drop": { "$exists": true } // Provjerava da li postoji polje 'drop' unutar 'o' objekta
 }).sort({
 "ts": -1 // Sortiraj po timestampu (najnoviji prvi)
 })

Nalazim timestamp **1749562420, i: 1**

Sada opet radimo zadnji restore backupa koji imamo i sav proces restorea.

Onda radimo restore iz oploga:

 ``mongorestore --host=mongors1:27017 -u BackupUser -p 123 --authenticationDatabase=admin --oplogReplay --oplogLimit 1749562420:1 oplogR``

Trebali bi sada imati vraćenu kolekciju.

=== 4.6 Skriptirani backup


=== 4.7 Upgrade na novu verziju mongodb instance u replika setu.

Pogledamo status replica seta jel sve zdravo.

Odradimo backup za svakii slučaj.

TIP: Upoznamo se s novim featursima u novoj verziji pogotovo s compatibilty changes.

Svi u setu moraju imati featureCompabilityversion istu, a da to provjerimo ukucamo na svakom:

 ``db.adminCommand( { getParameter: 1, featureCompatibilityVersion: 1 } )``

Svi nodovi moraju biti u zdravom stanju a da to provjerimo:

 ``db.adminCommand( { replSetGetStatus: 1 } )``

Kad smo odradili pripreme mozemo poceti s nadogradnjom, prvo spustimo jedan sekundarni:

 ``db.adminCommand( { shutdown: 1 } )``

Dodamo u yaml file novi image i pokrenemo novu instancu, trebala bi se automatski dodati u replica set.

Ponovimo istu komandu za provjeru statusa i obavezno pregledati logove da se synca uredno.

Ovdje valja dodati da vidis da slucajno neki node nema drift ili kasnjenje u replikaciji:

 ``db.printSecondaryReplicationInfo()``


Isto uradimo i s drugim nodom

Kad dodjemo do primaryu njega moram stepDown da odstupi s te pozicije da bi i njega mogli spustiti kao secundary:

``rs.stepDown(60)``

I njemu radimo isti korak upgradea

Kad je sve gotovo spojimo se na primary gdje cemo potvrditi da smo sigurni da krecemo s novim featursima pomocu(u realnom vremenu pricekamo par dana da vidimo da necemo ici downgrade verzije posto poslije ovoga jedini je downgrade da zovemo direkt u mongo):

``db.adminCommand( { setFeatureCompatibilityVersion: "7.0" } )``

=== 4.8 Indexi u Mongodb

    ``db.kolekcija.getIndexes()``  # da vidimo indexe koje ima kolekcija

Defaultno imamo index na polju _id.

**Single field** index:

    ``db.test.createIndex ({ age: 1 }) `` # ako je 1 asc a ako je -1 onda je kreiran kao descending

    age_1 # index dobije unique ime ako mu mi to ne definiramo

Mozemo koristiti komandu explain('executionStats' za detaljniji uvid) da vidimo da li query koristi index scan ili collection scan

    ``db.test.find({ age: { $gt: 30 } }).explain()``

**Compound** index:

    ``db.test.createIndex ({name: 1, age: 1})``

Compound index moze biti puno efektivniji od single field indexa, jos ne moramo imati vise indexa nego samo jedan. Redoslijed s ljeva na desno je vazan da bi index bio efektivniji i pokrio query.A definiramo ga pomocu ESR (Equality, Sort, Range).

Postoje još:

MultiKey index:

Indexi koji se kreiraju na polju koje ima niz.

Hashed index:

Pomazu nam pri shardiranju podataka na vise servera.

Postoje jos text indexi , geospatial...

Opcije koje mozemo primijeniti na postojece indexe "unique", "sparse", "TTL"...

=== 4.9 Hvatanje SLOW QUERIA


    ``db.getProfilingStatus()``

0 - ne hvata nikakvu datu

1 - hvata samo operacije koje prelaze slowms: 100 (stavljeno po defaultu)

2 - hvata sve operacije (nije preporucljivo, defaultna velicina 1MB)

    ``db.setProfilingLevel(1)``

Stvara se nova kolekcija system.profile i na njoj mozemo izvrsavati upite.

   `` db.system.profile.find()``

=== 4.10 Logovi Mongodb

Understand MongoDB log messages
Now let's take a closer look at the log messages. Each log entry is a JSON object with the following fields:

"t": Records the timestamp of the log message in ISO-8601 format.
"s": The severity level of the log message.
"c": Specifies the component this log record belongs to.
"id": A unique identifier.
"ctx": Context information.
"msg": The message body.
"attr": Include additional information such as client data, file path, line number, etc.
"tags": Optional tags.
"truncated": Contains truncation information if the message is truncated.
"size": The original size of the entry before truncation.

MongoDB provides the following verbosity levels:

"F": Fatal messages.
"E": Error messages.
"W": Warning messages.
"I": Informational messages. Corresponds to numeric value 0.
"D1"-"D5": Debug messages, Corresponds to numeric value 1-5.

-Logove mozemo lakse citati sa jq toolom

``sudo cat mongod.log | jq``

-U **/etc/logrotate.d/** */ napravimo skriptu(nazovemo je mongodb) za rotiranje logova

[source, bash]
/home/tonia/DockerMongoRS/log1/mongod.log {
daily               # jednom dnevno radi rotaciju ako hocemo neko drugo vrijeme osim(daily, weekly, monthly,yearly) pravimo novi cron job
size 100m           # ili je file veci od 100 MB a provjerava se jednom dnevno
rotate 7            # cuva maksimalno 7 log fileova   
missingok           # ako log ne postoji ok je   
notifempty          # nece praviti novi ako je stari log prazan
compress            # raditi ce kompresiju loga   
delaycompress       # nece raditi kompresiju "predzadnjeg loga"   
create 0644 999 999 # kreira novi log s ovim dozvolama i permisijama
sharedscripts       # skriptu ce okinuti samo jednom (valja kad ima vise log fileova u jednom folderu
postrotate          # pocetak skripte
  docker exec mongors1 mongosh -u admin -p ass --eval 'db.adminCommand({logRotate: 1})' >/dev/null 2>&1  # sadrzaj skripte
endscript      # kraj skripte
}

-Pomocu  mozemo forsirati izvodjenje skripte da testiramo:

``sudo logrotate -f /etc/logrotate.d/mongodb``

-Sad ovo uradimo i sa ostala 2 noda

== 4.11 How to Delete bunch of data and reclaim space in MongoDB

* link:https://www.percona.com/blog/using-compact-in-percona-server-for-mongodb-from-version-4-4/[Compact]

Prvo unesemo veću količinu podataka:

[source,javascript,collapsible="Prikaži skriptu"]
.Kopiraj ovaj kod u `mongosh` konzolu
----
// --- POMOĆNE FUNKCIJE ZA GENERIRANJE SLUČAJNIH PODATAKA ---

function getRandomInt(min, max) {
    return Math.floor(Math.random() * (max - min + 1)) + min;
}

function getRandomFloat(min, max, decimals) {
    var factor = Math.pow(10, decimals || 2);
    return Math.round((Math.random() * (max - min) + min) * factor) / factor;
}

function getRandomDate(startYear, endYear) {
    var start = new Date(startYear || 1950, 0, 1).getTime();
    var end = new Date(endYear || 2000, 11, 31).getTime();
    return new Date(getRandomInt(start, end));
}

function getRandomString(length) {
    var chars = 'abcdefghijklmnopqrstuvwxyz';
    var str = '';
    for (var i = 0; i < length; i++) {
        str += chars.charAt(getRandomInt(0, chars.length - 1));
    }
    return str;
}

function getRandomEmail(name) {
    var domains = ['example.com', 'test.org', 'mail.com', 'sample.net'];
    return name.toLowerCase() + '@' + domains[getRandomInt(0, domains.length - 1)];
}

function getRandomTags() {
    var tagPool = ['new', 'customer', 'vip', 'banned', 'beta', 'internal', 'review'];
    var tagCount = getRandomInt(1, 4);
    var tags = [];
    for (var i = 0; i < tagCount; i++) {
        tags.push(tagPool[getRandomInt(0, tagPool.length - 1)]);
    }
    return tags;
}

// --- GLAVNA SKRIPTA ZA GENERIRANJE I UNOS PODATAKA ---

// Podesive varijable za unos
var batchSize = 10000;
var totalDocuments = 10000000; // Ja sam prekinuo na 4 milijuna

// Inicijalizacija niza za serijski unos i brojača
var batch = [];
var count = 0;

// Glavna petlja za generiranje i unos podataka
for (var i = 1; i <= totalDocuments; i++) {
    // Stvori novi dokument
    var name = "name" + i;
    var birthday = getRandomDate(1950, 2000);
    var age = new Date().getFullYear() - birthday.getFullYear();

    var doc = {
        name: name,
        age: age,
        birthday: birthday,
        email: getRandomEmail(name),
        address: {
            street: getRandomInt(100, 9999) + ' ' + getRandomString(8) + ' St',
            city: getRandomString(6),
            zip: getRandomInt(10000, 99999),
            country: getRandomString(7)
        },
        isActive: Math.random() < 0.5,
        tags: getRandomTags(),
        score: getRandomFloat(0, 100, 2),
        createdAt: new Date(),
        metadata: {
            imported: Math.random() < 0.3,
            source: ['api', 'form', 'manual'][getRandomInt(0, 2)]
        }
    };

    // Dodaj dokument u seriju (batch)
    batch.push(doc);

    // Ako je serija puna, pošalji je bazi i ispiši napredak
    if (batch.length === batchSize) {
        db.useri.insertMany(batch);
        count += batchSize;
        print("Inserted " + count + " documents...");
        batch = []; // Resetiraj seriju
    }
}

// Ubaci preostale dokumente iz posljednje, nepotpune serije
if (batch.length > 0) {
    db.useri.insertMany(batch);
    count += batch.length;
    print("Inserted a final batch, total documents: " + count);
}
----

Možemo provjeriti stanje s ``db.stats()``

[source,javascript,collapsible="Prikaži skriptu"]
.Kopiraj ovaj kod u `mongosh` konzolu
----
db.stats()
{
  db: 'test_db',
  collections: Long('1'),
  views: Long('0'),
  objects: Long('5120000'),
  avgObjSize: 326.171262109375,
  dataSize: 1669996862,
  storageSize: 633901056,
  indexes: Long('1'),
  indexSize: 142004224,
  totalSize: 775905280,
  scaleFactor: Long('1'),
  fsUsedSize: 8918220800,
  fsTotalSize: 214736809984,
  ok: 1,
  '$clusterTime': {
    clusterTime: Timestamp({ t: 1758539352, i: 10000 }),
    signature: {
      hash: Binary.createFromBase64('ROkuemj6hprMelfN/uQKg41C9Bg=', 0),
      keyId: Long('7552854132097286149')
----

Sada ćemo pobrisati sve korisnije koji imaju manje od 50 godina:

[source,javascript,collapsible="Prikaži skriptu"]
.Kopiraj ovaj kod u `mongosh` konzolu
----
// Postavi na kolekciju u kojoj se nalaze podaci
var coll = db.useri;

// Definiraj dobnu granicu
var ageCutoff = 50;
var batch = 10000;

while (true) {
    // Pronađi ID-ove dokumenata čija je dob manja od 50 godina
    var ids = coll.find(
        { age: { $lt: ageCutoff } },
        { _id: 1 }
    )
    .limit(batch)
    .toArray()
    .map(function (d) { return d._id; });

    // Ako nema više dokumenata za brisanje, prekini petlju
    if (ids.length === 0) {
        print("Done. No more documents found.");
        break;
    }

    // Obriši pronađene dokumente
    var res = coll.deleteMany({ _id: { $in: ids } });
    print("Deleted: " + res.deletedCount + " documents.");

    // Ako je obrisan manji broj od veličine batcha, gotovi smo
    if (ids.length < batch) {
        print("Last partial batch. Done.");
        break;
    }
}
----

CAUTION: Pokreni compact uvijek na sekundarnim (secondary) ili skrivenim (hidden) čvorovima, ili na čvorovima s niskim prioritetom. Primarni čvor (Primary) obradi zadnji, nakon što ga prethodno degradiraš (step-down).
U replica setu, compact naredba mora biti pokrenuta na svakom čvoru.

Kada se podaci pobrišu opet možemo provjeriti ``db.stats()``.

I možemo provjeriti koliko ćemo prostora dobiti s compact naredbom:

``db.useri.stats().wiredTiger["block-manager"]["file bytes available for reuse"]``

Onda upotrijebimo:

[source,javascript,collapsible="Prikaži skriptu"]
.Kopiraj ovaj kod u `mongosh` konzolu
----
db.runCommand({ compact: 'useri', force: true });
{
  bytesFreed: 129789952,
  ok: 1,
  '$clusterTime': {
    clusterTime: Timestamp({ t: 1758540622, i: 1 }),
    signature: {
      hash: Binary.createFromBase64('6F/ouZDJi88sWoxJig7Ijs4y/Ik=', 0),
      keyId: Long('7552854132097286149')
    }
  },
  operationTime: Timestamp({ t: 1758540622, i: 1 })
}
----

Gdje smo oslobodili oko 130MB prostora.



CAUTION: Ponekad, kada se velika kolekcija komprimira, naredba compact odmah vraća OK, ali u stvarnosti, fizički prostor kolekcije ostaje nepromijenjen. To se događa jer WiredTiger smatra da kolekciju ne treba komprimirati. Da biste to prevladali, trebate ponovno pokretati naredbu compact dok se prostor ne oslobodi.








== 5.0 Kubernetes PXC Operator

Da se pridružimo novom clusteru moramo ga dodati u kube config. Skinemo yaml file od clustera i dodamo ga:

``KUBECONFIG=~/.kube/config:/home/tonia/Downloads/ime-clustera.yaml kubectl config view --flatten > ~/.kube/config-merged && mv ~/.kube/config-merged ~/.kube/config``

Na njega se možemo prebaciti pomoću kubectx ili komande:

``kubectl config use-context "ime-clustera"


=== 5.1 Kubectl deploy

* link:https://docs.percona.com/percona-operator-for-mysql/pxc/kubectl.html[Tutorial]

* link:https://github.com/percona/percona-xtradb-cluster-operator[Yaml fileovi]

Deploying a Percona XtraDB Cluster 

Added a new cluster to my kubeconfig file using the following commands:


``KUBECONFIG=~/.kube/config:/home/tonia/Downloads/staging-ubercluster-rke2.yaml kubectl config view --flatten > ~/.kube/config-merged && mv ~/.kube/config-merged ~/.kube/config``

Then switched to the new cluster's context:

``kubectl config use-context "staging-ubercluster-rke2"``



Deployed the Percona XtraDB Cluster Operator:


``kubectl apply --server-side -f https://raw.githubusercontent.com/percona/percona-xtradb-cluster-operator/refs/heads/main/deploy/operator.yaml -n dba-pxc-test``


Deployed the cluster without HAProxy first:


``kubectl apply -f pxc-ha.yaml -n dba-pxc-test``


Edited pxc-ha.ya7ml and deployed HAProxy as well:


``kubectl apply -f pxc-ha.yaml -n dba-pxc-test``

Get acces to databases:

``kubectl get secret pxc-db-secrets -n dba-pxc-test -o jsonpath='{.data.root}' | base64 --decode``

Tested port forwarding to the cluster:

``kubectl port-forward pxc-db-haproxy-0 3306:3306 -n dba-pxc-test``        * error, use service/nameofservice dont use pod name

This also worked on port 3307.


Cleanup and Troubleshooting

Eventually deleted everything by following the provided instructions.


PerconaPercona Operator for MySQL - Delete the Operator 


Ran into an issue where an old cluster was stuck and couldn't be deleted. I had to manually edit its YAML file and remove the finalizers to force its deletion.


``kubectl edit pxc cluster1 -n dba-pxc-test``


A similar problem happened with one of the certificates, which kept reappearing.



=== 5.2 Helm deploy

``mkdir KubernetesHelm``

``cd KubernetesHelm``

``helm repo add percona https://percona.github.io/percona-helm-charts/``

``helm repo update``



``helm pull percona/pxc-operator --version 1.16.1``

``helm pull percona/pxc-db --version 1.16.1``



edited **values.yaml** for both



From pxc-operator folder:

``helm install pxc-operator . -n dba-pxc-test``   

(didnt work because cant install crds, jj helped and bring it up)



From pxc-db folder:

``helm install pxc-db . -n dba-pxc-test``


Port forward worked


Cleanup again, now with helm:


``helm uninstall pxc-db -n dba-pxc-test``


``helm uninstall pxc-operator -n dba-pxc-test``


and deleted pvcs and secrets with kubectl.

































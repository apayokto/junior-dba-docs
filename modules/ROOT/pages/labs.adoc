= Praktične vježbe

Dobrodošli u praktični dio!

U ovoj ću sekciji podijeliti svoje osobno iskustvo s postavljanjem, konfiguracijom i upravljanjem replikacijskim setovima. Sve vježbe izvodio sam lokalno koristeći Docker i Docker Compose, što mi je omogućilo da simuliram produkcijsko okruženje bez utjecaja na svoj sustav.

Zapamti, iako sam ja pratio preporučene korake, ovo je tvoje igralište za učenje. Uvijek postoji više načina da se dođe do istog cilja, pa te ohrabrujem da istražuješ i druge opcije koje smatraš boljima ili zanimljivijima.

IMPORTANT: Logovi su ti najbolji prijatelj.

== 1. MySQL asinkrona replikacija

Za bolje razumijevanje asinkrone replikacije, možete pogledati ovaj link:https://www.digitalocean.com/community/tutorials/how-to-set-up-replication-in-mysql[Digital Ocean tutorial] (nije namijenjen za Docker, ali je vrlo koristan).


Započnite s pripremom projektnog direktorija. Kreirajte novi folder u vašem **~** direktoriju i nazovite ga npr. **DockerAR**.
Kada pokrenete **docker-compose**, ime foldera će automatski postati naziv link:https://docs.docker.com/compose/how-tos/project-name/[projekta].
Kreiramo datoteku config i u njoj pravimo master.cnf i slave.cnf sa varijablama potrebnim za asinkronu replikaciju.
[source, cnf]
----
# Na master serveru
[mysqld]
log_bin = mysql_bin
server_id = 1
binlog_format = ROW
expire_logs_days = 7
# Na slave serveru
[mysqld]
server_id = 2
relay_log = mysql_relay_bin
log_replica_updates = ON 
----

I još kreiramo foldere za logove(logmaster i logslave).
Da bi mysql iz kontenjera mogao u njih pisati i čitati moramo mu dati potrebne dozvole.(chown i chmod)
Koja grupa je mysql možemo saznati pomoću komande
``docker exec --user mysql <ime_kontejnera> id``

sudo chown -R 1001:1001 master_data 

Zatim kreirajte file **docker-compose.yaml** unutar tog foldera. Dodajte sljedeći sadržaj za definiranje servisa:

[source, yaml]

services:
  master:              # definiramo prvi kontenjer
    image: mysql:8.0  # vucemo image s docker huba s tagom 8.0
    container_name: master  # dajemo ime kontenjeru za lakse referenciranje
    ports:
      -  "33063:3306"  # exposamo port 3306 iz kontenjera na port 33063 na hostu
    volumes:                             # stvaramo *bind* volumene
      -  ./master_data:/var/lib/mysql    # bindamo mysql datu na hosta da ostane persistana
      -  ./config/master.cnf:/etc/mysql/conf.d/master.cnf  # bindamo mu da koristi nasu custom konfiguraciju
      -  ./logmaster:/var/log/mysql      # bindamo logove da ih mozemo citati s hosta
    environment:
      MYSQL_ROOT_PASSWORD: 1234         # root useru dajemo password
  slave:             # definiramo drugi kontenjer
    image: mysql:8.0      
    container_name: slave
    ports:
      -  "33064:3306"
    volumes:
      -  ./slave_data:/var/lib/mysql
      -  ./config/slave.cnf:/etc/mysql/conf.d/slave.cnf
      -  ./logslave:/var/log/mysql
    environment:
      MYSQL_ROOT_PASSWORD: 4321
    depends_on:                  # da se ne pokrece ako master nije pokrenut
      -  master


TIP: Vrste link:https://docs.docker.com/engine/storage/bind-mounts/[volumena] u dockeru.

Spajamo se na master server(``docker exec -it master bash``) i pravimo usera s grantovima potrebnim za replikaciju:
[source, mysql]
mysql> CREATE USER 'replica_user'@'%' IDENTIFIED WITH mysql_native_password BY 'password';
Query OK, 0 rows affected (0.02 sec)
mysql> GRANT REPLICATION SLAVE ON *.* TO 'replica_user'@'%';
Query OK, 0 rows affected (0.01 sec)
mysql> FLUSH PRIVILEGES;
Query OK, 0 rows affected (0.01 sec)

Unesemo nešto podataka i uradimo backup s mysqldump na masteru i restore na slaveu.

``mysqldump -u root -p --all-databases > backup.sql``

Kopiramo backup iz master kontenjera u slave.

``docker cp master:/backup.sql .`` ``docker cp ./backup.sql slave:/tmp``

Uradimo restore i provjerimo jesu li podaci tu.

mysql -u root -p < backup.sql


Spojimo se na mastera opet i nađemo poziciju binlogova:

[source, mysql]
mysql> show master status\G
*************************** 1. row ***************************
             File: mysql-bin.000012
         Position: 2400
     Binlog_Do_DB: 
 Binlog_Ignore_DB: 

Onda idemo na slave server i kucamo:
[source, mysql]
CHANGE MASTER TO
  MASTER_HOST = 'master',
  MASTER_USER = 'replica_user',
  MASTER_PASSWORD = 'password',
  MASTER_LOG_FILE = 'mysql-bin.000012',
  MASTER_LOG_POS = 2400;

[source, mysql]
mysql> start slave;
Query OK, 0 rows affected, 1 warning (0.05 sec)
mysql> show slave status\G
*************************** 1. row ***************************
               Slave_IO_State: Waiting for source to send event
                  Master_Host: master
                  Master_User: replica_user
                  Master_Port: 3306
                Connect_Retry: 60
              Master_Log_File: mysql-bin.000009
          Read_Master_Log_Pos: 923
               Relay_Log_File: mysql-relay-bin.000002
                Relay_Log_Pos: 1012
        Relay_Master_Log_File: mysql-bin.000009
             Slave_IO_Running: Yes
            Slave_SQL_Running: Yes...


U outputu za slave status vidimo da oba threada rade.
Možemo testirati da ubacimo nešto podataka na master i vidimo hoće li se replicirali na slavea.
U idućim vježbama susretati ćemo se još sa asinkronom replikacijom pa smo neke stvari ovdje izostavili.




== 2. Percona sinkrona replikacija

* link:https://docs.percona.com/percona-xtradb-cluster/8.4/docker-compose.html#directory-structure[Xtradb cluster docs]

Tu ima detaljan vodič kako dignuti cluster, al' sam ja na ovome primjeru učio šta rade varijable koje možemo koristiti u cnf fileovima.
Primjer cnf filea za node 1 kojega ćemo koristiti za ovaj cluster:


*server-id=1*: Jedinstveni identifikator za svaki MySQL poslužitelj u replikacijskom nizu.

*datadir=/var/lib/mysql*: Mapa gdje se nalaze sve baze podataka i tablice.

*log-error=/var/log/mysql/error.log*: Putanja do datoteke s bilješkama o pogreškama i upozorenjima.

*log_slave_updates=1*: Omogućuje da se promjene primljene s drugih servera bilježe u njegov binarni log. To je ključno za postavljanje replikacije u lancu.

*pxc-encrypt-cluster-traffic=OFF*: Isključuje enkripciju prometa između čvorova klastera.

*pxc_strict_mode=PERMISSIVE*: Dopušta korištenje eksperimentalnih varijabli, ali bilježi upozorenja u logove.

*bind-address=0.0.0.0*: Govori MySQL-u da sluša veze na svim mrežnim sučeljima. Umjesto *, koristi se 0.0.0.0.

*binlog_format=ROW*: Osigurava stabilnu Galera sinkronu replikaciju bilježenjem stvarnih promjena redaka u binarni log.

*gtid_mode=ON*: Uključuje Global Transaction ID za svaku transakciju, što pojednostavljuje replikaciju i oporavak.

*enforce_gtid_consistency=ON*: Strogo provodi GTID dosljednost, osiguravajući da su sve transakcije sigurne za GTID replikaciju.

*binlog_expire_logs_seconds=604800*: Automatski briše binarne logove starije od 7 dana.



*wsrep_provider=/usr/lib64/galera4/libgalera_smm.so*: Putanja do biblioteke koja implementira Galera replikacijski protokol.

*wsrep_cluster_address=gcomm://*: Adrese članova klastera. Ako je prazno (gcomm://), znači da je ovo prvi čvor (prvi koji se pokreće) i on uspostavlja klaster.

*wsrep_slave_threads=2*: Broj niti koje se koriste za primjenu promjena na čvoru, što ubrzava sinkronizaciju klastera.

*wsrep_log_conflicts=ON*: U logove bilježi konflikte koji nastaju u transakcijama.

*wsrep_node_address=node1*: IP adresa ili ime hosta ovog čvora.

*wsrep_cluster_name=pxc-cluster*: Ime klastera. Svi čvorovi s istim imenom čine klaster.

*wsrep_node_name=node1*: Jedinstveno ime čvora, koje se koristi za lakšu identifikaciju u logovima.

*wsrep_sst_method=xtrabackup-v2*: Metoda kojom se radi State Snapshot Transfer (SST). To je proces sinkronizacije novog čvora. xtrabackup-v2 je najčešća metoda.

*wsrep_provider_options="gcache.size=1G;gcs.fc_limit=10;gcs.fc_factor=0.8;"*: Varijable specifične za Galera replikaciju.Služe za kontrolu replikacije i stabilnost klustera.



*innodb_buffer_pool_size=1G*: Jedna od najvažnijih varijabli za performanse. Određuje veličinu memorije za keširanje podataka i indeksa.

*innodb_log_buffer_size=8M*: Određuje veličinu privremenog spremnika prije nego što se logovi transakcija sinkroniziraju na disk.

*innodb_log_file_size=128M*: Veličina pojedinačne datoteke redo loga.

*innodb_autoinc_lock_mode=2*: Način zaključavanja za AUTO_INCREMENT stupce. Vrijednost 2 omogućuje veću konkurentnost.

*innodb_file_per_table=1*: Svaka tablica ima svoju .ibd datoteku, što olakšava upravljanje i oslobađanje prostora.

*innodb_flush_log_at_trx_commit=2*: Određuje kada se logovi transakcija zapisuju na disk. Vrijednost 2 omogućuje da izgubimo podatke u rasponu od jedne sekunde, ali značajno poboljšava performanse.

*innodb_flush_method=O_DIRECT*: Metoda kojom se logovi zapisuju direktno na disk, zaobilazeći OS cache, što poboljšava I/O performanse.


Teorijsko objašnjavanje svake varijable je korisno, ali prava istina je da se Galera i MySQL konfiguracijske varijable najbolje uče kroz praksu.

Razne situacije zahtijevaju različite konfiguracije, a najbolje je testirati kako se klaster ponaša nakon svake promjene. Eksperimentiranje u kontroliranom okruženju, poput Docker kontejnera, omogućuje ti da sigurno istražuješ kako svaka varijabla utječe na performanse i stabilnost.

Svakako preporučujem da nastaviš istraživati i testirati druge varijable, posebno one unutar wsrep_provider_options, jer one imaju najveći utjecaj na ponašanje Galera klastera. Vremenom ćeš steći duboko razumijevanje koje ti ni jedna teorija ne može pružiti.

=== 2.0.1 Bootstrapanje noda

Bootstrapanje je proces pokretanja Galera klastera od nule. To je prva i najvažnija akcija koja se radi kad se želi dignuti klaster, bilo nakon prvog postavljanja, bilo nakon katastrofe (poput vraćanja iz backupa).

Kada se bootstrapa, jedan čvor (node) se pokreće kao "izvor istine". On je privremeni, jedini aktivni član klastera i označava početnu točku za sve replikacije. Ostali čvorovi će se spajati na njega, preuzimati njegovo stanje, i tek tada se klaster smatra funkcionalnim.

grastate.dat je datoteka koju Galera koristi za pohranu informacija o stanju čvora. Galera prati tri ključna podatka:

UUID (Unique Universal ID): Jedinstveni identifikator klastera.

Seqno (Sequence Number): Redni broj zadnje transakcije koju je taj čvor primio.

safe_to_bootstrap: 0 znači da ovaj čvor nije siguran za pokretanje novog klastera. 1 znači da se može bootsrapati s ovog nodea.

Galera koristi ove informacije pri pokretanju čvora da bi odlučila što treba učiniti.

Ako je UUID i Seqno isti kao na drugim čvorovima u klasteru, on se jednostavno spaja i replicira promjene.

Ako se razlikuje, on ulazi u proces State Snapshot Transfer (SST). To je proces u kojem traži od drugog čvora da mu pošalje kompletnu kopiju svih podataka, što se i događa u tvom restore scenariju.



=== 2.1 Testiranje pomoću sysbencha

U docker-compose.yaml dodati servis za sysbench:

[source, yaml]
tools:
    image: debian:12
    container_name: sysbenchtool2
    command: >
      bash -c "apt-get update && apt-get install -y sysbench && sleep infinity"

Na jednom nodu pravimo usera i bazu za testiranje:

[source, sql]
mysql> CREATE USER 'sbtest'@'%' IDENTIFIED BY 'pamet';
       GRANT ALL ON sbtest.* TO 'sbtest'@'%';
       CREATE SCHEMA sbtest;


Dignemo servis tools sa sysbenchom i počnemo testiranje:

[source, linux]
----
sysbench /usr/share/sysbench/oltp_write_only.lua --mysql-host=node1 --mysql-port=3306 --mysql-user=sbtest --mysql-password=pamet --mysql-db=sbtest --table_size=100000 --tables=10 --threads=25 --histogram=on --time=30 prepare

sysbench /usr/share/sysbench/oltp_write_only.lua --mysql-host=node1 --mysql-port=3306 --mysql-user=sbtest --mysql-password=pamet --mysql-db=sbtest --table_size=100000 --tables=10 --threads=25 --histogram=on --time=30 run

sysbench /usr/share/sysbench/oltp_write_only.lua --mysql-host=node1 --mysql-port=3306 --mysql-user=sbtest --mysql-password=pamet --mysql-db=sbtest --table_size=100000 --tables=10 --threads=25 --histogram=on --time=30 cleanup
----

Prepare unosi podatke u tablice, run odrađiva transakcije a cleanup čisti sve iza.

=== 2.2 Dodavanje asinkronog nodea preko GTID

Dodamo servis u docker-compose.yaml za asinkroni node:

[source, yaml]
  areplica:
    image: percona/percona-xtradb-cluster:8.0.41
    container_name: areplica
    deploy:
      resources:
        limits:
          cpus: '1'
          memory: 4G
    volumes:
      -  ./areplica_data:/var/lib/mysql
      -  ./config/areplica.cnf:/etc/mysql/conf.d/my.cnf
      -  ./logar:/var/log/mysql
    environment:
      MYSQL_ROOT_PASSWORD: 4GwihtremRDcQF

U konfiguracijski file za asinkroni node stavimo:

[source, cnf]
----
[mysqld]

server-id=4
bind_address=0.0.0.0
datadir=/var/lib/mysql
relay-log=/var/log/mysql/mysql-relay-bin.log
log-error=/var/log/mysql/error.log
binlog_format=ROW 
binlog_expire_logs_seconds=604800 
gtid_mode=ON
enforce_gtid_consistency=ON 
log_slave_updates=1
innodb_buffer_pool_size=1G
innodb_log_buffer_size=8M
innodb_log_file_size=128M
innodb_autoinc_lock_mode=2
innodb_file_per_table=1
innodb_flush_log_at_trx_commit=1
innodb_flush_method=O_DIRECT
read_only=ON
----

Dignemo asinkroni node i onda radimo fizički backup i restore pomoću xtrabackupa.

Za svaki slučaj obrišemo datu asinkronom nodu da restore može biti čist.

``sudo rm -rf asinkrona_data/*``

Na masteru radimo backup:

/usr/bin/pxc_extra/pxb-8.0/bin/xtrabackup --backup --target-dir=/tmp/backup --user=root --password=4GwihtremRDcQF --slave-info``

Možemo sherati bindani folder tako da ne moramo prebacivati backup, samo u volumes nodu gdje radimo backup i asinkronoj replici dodamo ovu liniju:

``-  ./backups:/tmp/backups``

Radimo restore pomoću komandi:

``/usr/bin/pxc_extra/pxb-8.0/bin/xtrabackup --prepare --target-dir=/tmp/backups``

``/usr/bin/pxc_extra/pxb-8.0/bin/xtrabackup --copy-back --target-dir=/tmp/backups``

Restoranoj dati moramo dati opet permisije:

``sudo chown -R 1001:1001 areplica_data``

Na nekom master nodu napravimo replika usera i onda na slaveu kucamo:

[source, mysql]
----
CHANGE MASTER TO
  MASTER_HOST = 'node1',
  MASTER_PORT = 3306,
  MASTER_USER = 'replica_user',
  MASTER_PASSWORD = 'password',
  SOURCE_DELAY=60,
  MASTER_AUTO_POSITION = 1;

start slave;
show slave status\G
----

Source_delay daje replikacijski delay od 60s. Što je korisno u nekim situacijama.

[TIP]
====
U nekim situacijama korisna je komanda, služi za preskakanje ako neka transakcija zapne, al se mora koristiti s oprezom pošto utječe na integritet podataka.

``STOP REPLICA;

SET GLOBAL SQL_SLAVE_SKIP_COUNTER = 1; 

START REPLICA;``
====



=== 2.3 Dodavanje Haproxya ispred instanci

U postojeći setup u docker-compose.yaml dodajemo image za haproxy:

[source, yaml]
haproxy:
    container_name: haproxy
    image: haproxy:3.2
    deploy:
      resources:
        limits:
          cpus: '1'
          memory: 1G
    volumes:
      -  ./config/haproxy.cfg:/usr/local/etc/haproxy/haproxy.cfg
    logging:
        driver: "local"
        options:
            max-size: "10m"
            max-file: "3"

I pravimo cfg file na gore pomenutoj lokaciji gdje ubacivamo:

[source, cfg]
----
global
    log stdout local0
    maxconn 4096
    daemon

defaults
    log global
    mode tcp
    option tcplog
    option dontlognull
    option log-health-checks
    option asap
    timeout connect 5000
    timeout client 50000
    timeout server 50000
    retries 3

frontend write
    bind *:3306
    mode tcp
    option clitcpka
    default_backend mysql_backend_write

frontend read
    bind *:3307
    mode tcp
    option clitcpka
    default_backend mysql_backend_read

backend mysql_backend_write
    mode tcp
    option srvtcpka
    balance leastconn
    server node1 node1:3306 check inter 2s rise 2 fall 3 weight 1
    server node2 node2:3306 check inter 2s rise 2 fall 3 weight 1 backup
    server node3 node3:3306 check inter 2s rise 2 fall 3 weight 1 backup

backend mysql_backend_read
    mode tcp
    option srvtcpka
    balance leastconn
    server node1 node1:3306 check inter 2s rise 2 fall 3 weight 1 backup
    server node2 node2:3306 check inter 2s rise 2 fall 3 weight 1 
    server node3 node3:3306 check inter 2s rise 2 fall 3 weight 1
  
----

TIP: Ako se konfiguracija piše u VSCodeu može se desiti da ne prođe jer zadnji red treba biti prazan da zna kada je kraj konfiguracije, pa pomoću nekog drugog txt editora dodamo prazan zadnji red pošto to ne očita u VSCodeu.


Ova konfiguracija omogućuje da writeovi uvijek idu na jedan node a readovi na druga dva.

Može se sa sysbench alatom testirati radi li.

=== 2.4 Upotreba Percona tool kita

Prvo ubacimo sliku percona tool kita u yaml file:
[source, yaml]
perconatool:
    image: perconalab/percona-toolkit:latest
    container_name: ptoolkit
    command: tail -F /dev/null   # s ovom komandom kontenjer stalno ostaje živ da mi možemo vježbati i istraživati


*pt-archiver*

Služi za arhiviranje podataka. Najveća mu je prednost što može raditi na 'živim' tablicama.
Vježbamo na istom percona setupu kao source a dignuti cemo mariadb cluster koji ce nam poslužiti kao destination. Pošto imamo samo sbtest bazu prebaciti ćemo jedan dio neke tablice u arhivu.
Možemo uzeti sbtest5 tablicu i prebaciti sve podatke koji imaju id manji od 2000 u arhivsku bazu.

Na source serveru kucamo:
``show create table sbtest5\G``

Na destination serveru pravimo bazu naziva **arhiva** i kopiramo create table što smo dobili iz prošle komande.

[source, mysql]
MariaDB [arhiva]> CREATE TABLE `arhiva_sbtest5` (
    ->   `id` int NOT NULL AUTO_INCREMENT,
    ->   `k` int NOT NULL DEFAULT '0',
    ->   `c` char(120) NOT NULL DEFAULT '',
    ->   `pad` char(60) NOT NULL DEFAULT '',
    ->   PRIMARY KEY (`id`),
    ->   KEY `k_5` (`k`)
    -> ) ENGINE=InnoDB AUTO_INCREMENT=300001 DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_0900_ai_ci;
Query OK, 0 rows affected (0.030 sec)

Na source smo provjerili i koliko imamo tih podataka da možemo potvrditi prebacivanje.
[source, mysql]
mysql> SELECT COUNT(*) FROM sbtest5 WHERE id < 2000;
+----------+
| COUNT(*) |
+----------+
|      667 |
+----------+

Ulazimo u kontenjer gdje nam se nalazi percona tool kit i istraživamo koje nam sve opcije trebaju da prebacimo podatke.

``bash-5.1$ pt-archiver --source 'h=node1,u=root,p=4GwihtremRDcQF,P=3306,D=sbtest,t=sbtest5' --where 'id < 2000' --dest 'h=172.22.0.2,u=root,p=JNC8D6HjtusQ5i,P=3306,D=arhiva,t=arhiva_sbtest5' --limit=30 --dry-run --statistics --no-check-columns --no-delete --retries=5 --sleep=1``

TIP: Uvijek prvo probamo s dry run, gdje će komanda proći sve korake ali neće izvršiti komandu.

[source, bash]
pt-archiver \
    --source 'h=node1,u=root,p=4GwihtremRDcQF,P=3306,D=sbtest,t=sbtest5' \
    --where 'id < 2000' \
    --dest 'h=172.22.0.2,u=root,p=JNC8D6HjtusQ5i,P=3306,D=arhiva,t=arhiva_sbtest5' \
    --limit=30 \
    --statistics \
    --no-check-columns \
    --retries=5 \
    --sleep=1

[source, mysql]
Started at 2025-09-12T10:21:45, ended at 2025-09-12T10:22:11
Source: D=sbtest,P=3306,h=node1,p=...,t=sbtest5,u=root
Dest:   D=arhiva,P=3306,h=172.22.0.2,p=...,t=arhiva_sbtest5,u=root
SELECT 667
INSERT 667
DELETE 667
Action         Count       Time        Pct
sleep             23    23.0064      85.48
commit          1336     3.0007      11.15
deleting         667     0.3710       1.38
inserting        667     0.3458       1.28
select            24     0.0322       0.12
other              0     0.1581       0.59

Možemo ući u mariadb da vidimo jesu tu podaci.
[source, mysql]
MariaDB [arhiva]> select count(*) from arhiva_sbtest5;
+----------+
| count(*) |
+----------+
|      667 |
+----------+

*pt-online-schema-change*

Ovaj alat služi za promjenu sheme tablice bez zaključavanja tablice i bez ikakvog zastoja za bazu. Idealan je za produkcijske sustave gdje se ne smije prekidati rad aplikacija.

U vježbi ćemo dodati novi stupac created_at u arhivsku tablicu, kako bismo znali kada su podaci prebačeni u arhivu.

Prvo, uđi u svoj MariaDB kontejner i provjeri trenutnu shemu tablice arhiva_sbtest5.
To je važno da se uvjeriš da stupac ne postoji i da ispravno ciljaš tablicu.

``MariaDB [arhiva]> show create table arhiva_sbtest5\G``

Sada, unutar ptoolkit kontejnera, pokreni pt-online-schema-change. Koristit ćemo --dry-run da vidimo hoće li sve proći bez problema, ali bez stvarne izmjene tablice.

[source, bash]
----
pt-online-schema-change \
    --alter "ADD COLUMN created_at TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP AFTER pad" \
    --host=maria1 \
    --user=root \
    --password=JNC8D6HjtusQ5i \
    --port=3306 \
    --database=arhiva \
    --table=arhiva_sbtest5 \
    --max-load Threads_running=50,Threads_connected=100 \
    --chunk-size=1000 \
    --check-interval=1 \
    --recursion-method=NONE \
    --dry-run
----

Nakon što se uvjeriš da je dry-run prošao bez greške, možeš pokrenuti pravu promjenu. Jednostavno ukloni --dry-run i dodaj opciju --execute.

[source, mysql]
MariaDB [arhiva]> show create table arhiva_sbtest5\G
*************************** 1. row ***************************
       Table: arhiva_sbtest5
Create Table: CREATE TABLE `arhiva_sbtest5` (
  `id` int(11) NOT NULL AUTO_INCREMENT,
  `k` int(11) NOT NULL DEFAULT 0,
  `c` char(120) NOT NULL DEFAULT '',
  `pad` char(60) NOT NULL DEFAULT '',
  `created_at` timestamp NOT NULL DEFAULT current_timestamp(),
  PRIMARY KEY (`id`),
  KEY `k_5` (`k`)
) ENGINE=InnoDB AUTO_INCREMENT=300001 DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_0900_ai_ci
1 row in set (0.001 sec)

=== 2.5 Dodavanje arbiter noda u cluster

=== 2.6 Indexi u MySQLu


== 3. MariaDB sinkrona replikacija

Dignuti cluster s mariadb node-a.Jedina razlika od Percona clustera je što u cnf fileu moramo dodati usera za sst backup ``wsrep_sst_auth=root:pass``  pa idemo odmah na iduću vježbu.

=== 3.1 Mariadb full fizički backup i restore

==== backup

* link:https://mariadb.com/docs/server/server-management/install-and-upgrade-mariadb/installing-mariadb/binary-packages/automated-mariadb-deployment-and-administration/docker-and-mariadb/container-backup-and-restoration[Maria-backup docs]

Odvajanje backup procesa u zasebnu komponentu (kontejner) omogućuje standardizaciju. Bez obzira na vrstu baze podataka (MySQL, MariaDB, MongoDB, itd.), backup kontejner uvijek radi na isti način. Spoji se na bazu, povuče podatke, i spremi ih na željenu lokaciju. To znatno pojednostavljuje automatizaciju i upravljanje infrastrukturom.

Iako se može činiti kao dodatni korak, korištenje zasebnog kontejnera za backup dugoročno smanjuje složenost, povećava sigurnost i stabilnost produkcijskog sustava. Zato je to best practice u modernim infrastrukturama.

Dignemo backup kontenjer, a yaml za njega smo stavili:
[source, yaml]
mariabackup:   # pravimo servis za backup
    image: mariadb:11.4   # vucem citav image mariadb posto ce imati toolse, a nisam mogao naci sliku samo s toolsima 
    container_name: mariabackup   # dajemo ime kontenjeru
    deploy:                       # ogranicivamo resurse
      resources:
        limits:
          cpus: '1'
          memory: 4G
    volumes:                     # mountamo volume od node2 u kontenjer za backup i volume gdje cemo cuvati backupe
      -  ./maria2_datanew:/var/lib/mysql:ro
      -  ./backups2:/var/backups
    command: ["sleep", "infinity"]   # kazem kontenjeru ako mu se ukine glavni proces(mariadb) da ipak ostane u njemu

Napravimo na nekom nodu usera za backup s potrebnim minimalnim privilegijama:

[source, mysql]
----
CREATE USER 'mariabackup'@'%'
 IDENTIFIED BY 'mbu_passwd';

 GRANT RELOAD, PROCESS, LOCK TABLES, REPLICATION CLIENT
 ON *.* TO 'mariabackup'@'%';
----

Uđemo u backup kontenjer i kucamo komandu:

``mariadb-backup --backup --host=maria2 -u mariabackup -p mbu_passwd --target-dir=/var/backups``

Sada imamo backup u folderu na hostu, jer smo bindali target dir.

Ovo je najjednostavniji oblik backupa, backupe još možemo npr. streamati i kompresirati u jedan file i slati u neki bucket, što može biti neka druga vježba u kombinaciji sa skriptiranjem backupa i stavljanjem crontabova za redovno okidanje.

==== restore

U praksi, restore najčešće koristimo u dva scenarija:

1. Testiranje ispravnosti backupa: Redovito vraćanje baze podataka na testnom poslužitelju jedini je siguran način da se provjeri radi li proces backupa ispravno. To je kritičan korak u osiguravanju da, u slučaju prave katastrofe, naša sigurnosna kopija zaista funkcionira i da su podaci pouzdani.

2. Stvarni oporavak od katastrofe (DR): Ako se dogodi najgore, ovaj proces nam omogućuje da vratimo sustav u operativno stanje koristeći posljednju ispravnu kopiju podataka.

Dakle, iako je restore operacija koju ne želimo izvoditi, njezino testiranje je obavezno. Dobra praksa je da se to radi redovito, čime se stječe sigurnost u ispravnost našeg procesa i spremnost za svaki scenarij.

Simuliramo katastrofu i spustimo sva 3 nodea.

Dignemo kontenjer za backup i restore i u njemo odmah mozemo restorati podatke pošto je data folder bindan za node na kojem ćemo raditi bootstrap u ovom slučaju node naziva maria2.

Prvo radimo prepare pa kopiranje fileova:

``mariadb-backup --prepare --target-dir=/var/backups``

``mariadb-backup --copy-back --target-dir=/var/backups``

Dajemo novoj dati prava da mariadb proces ima prava koristiti i čitati novu datu ``sudo chown -R 1001:1001 maria2_datanew`` 

Sada pripremimo node koji je dijelio datu za bootstrap. To jest da pravi novi cluster koji će ostalima biti novi izvor istine.

Komandu za boostrap u yaml fileu stavljamo na njega. A ostalima provjerimo da su ``Safe to bootstrap: 0`` u grastate datoteci.

Dižemo i druga dva noda koji će se pomoću SST spojiti i formirati novi cluster od 3 nodea.



=== 3.2 Mariadb incremental fizički backup i restore

Glavni razlog zašto koristimo inkrementalne backupe je učinkovitost. Dok je puni backup (full backup) ključna polazna točka, njegovo svakodnevno ponavljanje može biti vrlo skupo u smislu vremena i prostora na disku. Zamisli da imaš bazu podataka od 500 GB. Svaki dan raditi novi, puni backup znači da ti treba dodatnih 500 GB prostora i sat vremena ili više da se završi. To jednostavno nije održivo.

Tu na scenu stupa inkrementalni backup. On kopira samo one promjene (novi redovi, ažuriranja, brisanja) koje su se dogodile od posljednjeg backupa, bez obzira je li taj bio puni ili inkrementalni. Ova metoda štedi:

Prostor na disku: Umjesto gigabajta, inkrementalni backup može biti samo nekoliko megabajta ili gigabajta, ovisno o aktivnosti baze.

Vrijeme: Proces je mnogo brži jer ima manje podataka za kopiranje.


Incremental backup ne mozemo raditi bez full backupa a imamo ga iz prijasnje vjezbe pa sad unesemo nesto novih podataka da imamo sta backupirati s incremental backupom.

Incremental backup opet radimo iz backup kontenjera:

``mariadb-backup --backup --target-dir=/var/backups/noviinc --incremental-basedir=/var/backups2 --host=maria2 --user=mariabackup --password=mbu_passwd``

target dir je direktorij gdje će biti sačuvan incr backup a base dir je direktorij od full backupa.

Sada opet spustamo sva 3 noda i brisemo im datu iz bindanih volumena.

Onda radimo prepare backupa, prvo full pa incremental:

``mariadb-backup --prepare --target-dir=/var/backups/novi``

``mariadb-backup --prepare --target-dir=/var/backups/novi --incremental-dir=/var/backups/noviinc``

TIP: Kad imamo više incrementalnih backupa tipa ponedjeljak, utorak, srijeda, četvrtak... Prvo prepare ponedjeljak pa utorak pa srijeda pa četvrtak...

Pa radimo restore:

``mariadb-backup --copy-back --target-dir=/var/backups/novi``

Dizemo node2 s novim izvorom istine i spajamo druga dva noda.

=== 3.3 Mariadb logički backup s Mydumperom

=== 3.4 Mariadb PITR(Point in time restore) proces


=== 3.5 Vježbanje skripti za backupe


=== 3.6 Postavljanje Maxscale proxya ispred clustera

=== 3.7 Dodavanje asinkronog nodea clusteru gdje konekcija ide preko maxscalea

=== 3.8 Dodavanje i proučavanje TLS certifikata za MariaDB instance

=== 3.9 PMM monitoring i dashboardi

== 4. MongoDB replika set

Prvo pisemo docker-compose.yaml file

Pravimo foldere za conf fileove koje cemo mountati kroz yaml i dodajemo u njih sta nam treba --> mongo1.cnf mongo2.cnf mongo3.cnf (2 i 3 su isto kao 1)

Pravimo foldere za datu koju cemo mountati kroz yaml --> data1_mongo data2_mongo data3_mongo (mkdir) // on ce i sam napraiviti foldere al nece imati permisije zato ih ja odmah napravim i dadnem permisije.

Pravimo foldere za logove koje cemo mountati kroz yaml --> log1 log2 log3 (mkdir)

Dajemo im permisije za pisanje i vlasništvo nad data i log folderima da to mongo instanca može i raditi iz dockera (chown, chmod) \ davao sam za chmod 777 da mi sigurno radi al u praksi ne treba tako

Pošto smo u conf enable security moramo sada napraviti key file i mountati ga premo yaml file u kontenjer za svaki node /etc/mongodb-keyfile

 ``openssl rand -base64 756 > ./mongodb-keyfile``  \\ ne znam sada gdje ga drzimo ja sam ga ostavio u ovom folderu
 ``chmod 0400 /etc/mongo-keyfile``  \\ moramo mu dati permisije samo za citanje inace ga mongo instanca nece prihvatiti
 ``chown -R 999:999 /etc/mongo-keyfile``

Sve dobro provjeriti i vidjeti u yaml file jesu li imena i fileovi dobro mountani \ posebno zbog čestih typo

Pomoću docker-compose podignuti sve 3 instance(!obavezno jednu po jednu!)

Spojiti se na jednu i inicializirati replica set:

[source, mongodb]
 rs.initiate(
 {
 _id: "rs0",  // id replica seta
 version: 1,
 members: [
          { _id: 0, host : "mongors1:27017" },
          { _id: 1, host : "mongors2:27017" },
          { _id: 2, host : "mongors3:27017" }         ] } )


Upotrijebiti admin database i dodati usera:

[source, mongodb]
use admin
db.createUser( { user: "admin", pwd: "ass", roles: [{ role: "root", db: "admin" }] })

Opet se logirati i provjeriti status replica seta:

``rs.status()``

Automatsko mijenjanje primary nodea,upišemo na primarnom nodu:

``rs.stepDown(60)``

Manualno mijenjaje primary nodea,na primarnom nodu upišemo:

[source, mongodb]
cfg = rs.conf()   // Trenutna konfiguracija replica seta
cfg.members[2].priority = 2  //stavimo mu veci prioritet tako znamo da ce on biti izabran
rs.reconfig(cfg)  // primjenjuje novu konfiguraciju
rs.stepDown(60)  // automatski mu je 60 sekundi ali stavimo svakako

Sada ce replica izabrati node s _id 2 da bude primarni posto smo njemu dali veci prioritet.

=== 3.1 Dodavanje Hidden noda u replika set


Hidden nam služi da radimo backupe ili analitiku preko njega da ne bi opterećivali ostale nodove.

Prvo ga dodamo u docker-compose.yaml

Napravimo i za njega mongoh.conf koji cemo mountati kroz yaml

Napravimo foldere za datu i log fileove koje cemo mountati u yamlu s permisijama --> mongohidden_data loghidden \ dati permisije i vlasnistva

I njemu moramo mountati key file(procedura opisana u prijasnjoj vježbi).

Dignemo ga s docker-compose

Spojimo se na primary instancu i pridružimo ga replika setu:

[source, mongodb]
 rs.add({
 host: "mongohidden:27017",  // njegova adresa
 hidden: true,               // da je skriven
 priority: 0,                // da ne moze biti izabran za primarnog
 votes: 0,                   // da ne moze glasati
 secondaryDelaySecs: 3600    // da pise podatke s zaostatkom od 1h, ako se slucajno pobrise nesto s primarnog da mozemo brze vratiti podatke 
 })

Provjeriti status, njemu bi trebalo pisati **hidden: true**

=== 3.2 Dodavanje Arbiter noda u replika set

Arbiter nam sluzi u slucaju parnog broja nodova da bude dodatni glas kojim se moze izabrati primarni (najčešći slučaj je 2 noda + arbiter)

Prvo ga dodamo u docker-compose.yaml file.

Njemu ne treba folder za datu, mozemo napraviti samo za logove --> logarbiter // i dati mu permisije i vlasnistva

Napraviti i mongoarb.conf file za njega, mozemo komentirati sve vezano za podatke a ostaviti postavke za replikaciju, i mountati ga u yamlu.

I njemu se mora mountati key file.

Dizemo ga s docker-compose

Pridruzimo ga replica setu s primarnog noda

[source, mongodb]
 rs.addArb({"mongoarb:27017"}) 

Sada mozemo provjeriti stanje:

 ``rs.status()``


=== 3.3 Backup s mongodump

Pravimo novi kontenjer 'mongobackup' s istom slikom kao i nodovi na kojima ce raditi backup --> docker-compose.yaml.

Mountamo novi folder gdje cemo cuvati backupe --> backups.

U enviromentu dodamo novog usera za pravljenje backupa.

Dignemo novi kontenjer pomocu docker-compose i spojimo se na njega.

Moramo napraviti usera i na replici setu da se može raditi backup, tako da se spojimo na primarnu instancu i kreiramo ga:

[source, mongodb]
 db.createUser(
 {
 user: "BackupUser",  
 pwd: "123", 
 roles: [ { role: "backup", db: "admin" } ,
         { role: "restore", db: "admin" } ]
 }
 )

Na bash liniji u kontenjeru za backup kucamo:

 ``mongodump --host=mongors2:27017 --username=BackupUser --password=123  --authenticationDatabase admin -readPreference=secondary --oplog --gzip --archive=/var/backups``

I imamo dump svih baza.

=== 3.4 Restore s mongodump

Prvo spustimo sva tri noda posto 'emo raditi "novi replica set" 

Pobrišemo svu datu iz sva tri noda.

Dignemo jedan node i na njemu uradimo inicijalizciju i dodavanje usera.

[source, mongodb]

 rs.initiate(
 {
 _id: "rs0",
 members: [
         { _id: 0, host : "mongors1:27017" } ] } )

 use admin

 db.createUser(
 {
 user: "BackupUser",                                      
 pwd: "123", // 
 roles: [ { role: "backup", db: "admin" } ,
         { role: "restore", db: "admin" } ]
 }
 )     


Iz kontenjera 'mongobackup' na bash liniji kucamo:

``mongorestore --archive=/var/backups/mongodump-2025-05-26.archive --gzip --host=mongors1:27017 -u BackupUser -p 123 --authenticationDatabase=admin``

Provjerimo jesu li podaci tu na prvom nodu al prvo se logiramo sa nasim userom.

``show dbs``

Ako jesu dižemo i druga dva noda s docker-compose.

I dodajemo ih u replika set.

[source, mongodb]
 rs.add('mongors2:27017')
 rs.add('mongors3:27017')

 Sada bi trebali imati sve podatke opet.


=== 3.5 Restore PITR

Scenario: U bazi smo napravili danas neku novu kolekciju i još nešto radili, i sad slučajno pobrišemo neku staru kolekciju jer je bila sličnog imena.

Moramo imati backup s oplogom i on mora sadržavati zajedničku točku s novim oplogom,tj. scoop oploga mora uhvatiti i backupov.

Uradimo dump oploga iz backup kontenjera:

 ``mongodump --host=mongors2:27017 --username=BackupUser --password=123   --authenticationDatabase admin -d local -c oplog.rs -o oplogD``

Premjestimo oplog u novi direktorij:

 ``mv oplogD/local/oplog.rs.bson oplogR/oplog.bson``

Sada mozemo procitati oplog file pomocu bsondump ili naći u bazi ako još imamo timestamp kada je dropana kolekcija.

Pošto u mene jos je živa mongo instanca provjeravam pomoću:
[source, mongodb]
 db.oplog.rs.find({
 "op": "c",
 "o.drop": { "$exists": true } // Provjerava da li postoji polje 'drop' unutar 'o' objekta
 }).sort({
 "ts": -1 // Sortiraj po timestampu (najnoviji prvi)
 })

Nalazim timestamp **1749562420, i: 1**

Sada opet radimo zadnji restore backupa koji imamo i sav proces restorea.

Onda radimo restore iz oploga:

 ``mongorestore --host=mongors1:27017 -u BackupUser -p 123 --authenticationDatabase=admin --oplogReplay --oplogLimit 1749562420:1 oplogR``

Trebali bi sada imati vraćenu kolekciju.

=== 3.6 Skriptirani backup


=== 3.7 Upgrade na novu verziju mongodb instance u replika setu.

Pogledamo status replica seta jel sve zdravo.

Odradimo backup za svakii slučaj.

TIP: Upoznamo se s novim featursima u novoj verziji pogotovo s compatibilty changes.

Svi u setu moraju imati featureCompabilityversion istu, a da to provjerimo ukucamo na svakom:

 ``db.adminCommand( { getParameter: 1, featureCompatibilityVersion: 1 } )``

Svi nodovi moraju biti u zdravom stanju a da to provjerimo:

 ``db.adminCommand( { replSetGetStatus: 1 } )``

Kad smo odradili pripreme mozemo poceti s nadogradnjom, prvo spustimo jedan sekundarni:

 ``db.adminCommand( { shutdown: 1 } )``

Dodamo u yaml file novi image i pokrenemo novu instancu, trebala bi se automatski dodati u replica set.

Ponovimo istu komandu za provjeru statusa i obavezno pregledati logove da se synca uredno.

Ovdje valja dodati da vidis da slucajno neki node nema drift ili kasnjenje u replikaciji:

 ``db.printSecondaryReplicationInfo()``


Isto uradimo i s drugim nodom

Kad dodjemo do primaryu njega moram stepDown da odstupi s te pozicije da bi i njega mogli spustiti kao secundary:

``rs.stepDown(60)``

I njemu radimo isti korak upgradea

Kad je sve gotovo spojimo se na primary gdje cemo potvrditi da smo sigurni da krecemo s novim featursima pomocu(u realnom vremenu pricekamo par dana da vidimo da necemo ici downgrade verzije posto poslije ovoga jedini je downgrade da zovemo direkt u mongo):

``db.adminCommand( { setFeatureCompatibilityVersion: "7.0" } )``

=== 3.8 Indexi u mongodb

    ``db.kolekcija.getIndexes()``  # da vidimo indexe koje ima kolekcija

Defaultno imamo index na polju _id.

**Single field** index:

    ``db.test.createIndex ({ age: 1 }) `` # ako je 1 asc a ako je -1 onda je kreiran kao descending

    age_1 # index dobije unique ime ako mu mi to ne definiramo

Mozemo koristiti komandu explain('executionStats' za detaljniji uvid) da vidimo da li query koristi index scan ili collection scan

    ``db.test.find({ age: { $gt: 30 } }).explain()``

**Compound** index:

    ``db.test.createIndex ({name: 1, age: 1})``

Compound index moze biti puno efektivniji od single field indexa, jos ne moramo imati vise indexa nego samo jedan. Redoslijed s ljeva na desno je vazan da bi index bio efektivniji i pokrio query.A definiramo ga pomocu ESR (Equality, Sort, Range).

Postoje još:

MultiKey index:

Indexi koji se kreiraju na polju koje ima niz.

Hashed index:

Pomazu nam pri shardiranju podataka na vise servera.

Postoje jos text indexi , geospatial...

Opcije koje mozemo primijeniti na postojece indexe "unique", "sparse", "TTL"...

=== 3.9 Hvatanje SLOW QUERIA


    ``db.getProfilingStatus()``

0 - ne hvata nikakvu datu

1 - hvata samo operacije koje prelaze slowms: 100 (stavljeno po defaultu)

2 - hvata sve operacije (nije preporucljivo, defaultna velicina 1MB)

    ``db.setProfilingLevel(1)``

Stvara se nova kolekcija system.profile i na njoj mozemo izvrsavati upite.

   `` db.system.profile.find()``

== 4.0 Kubernetes PXC Operator

=== 4.1 Kubectl deploy

=== 4.2 Helm deploy

































